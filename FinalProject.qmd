---
title: "Data Mining"
author: "Leona Hasani, Leona Hoxha, Nanmanat Disayakamonpan"
format: html
html:
code-fold: true
embed-resources: true
---

# PROJECT OVERVIEW
## Introduction, Goal, and Objective
In the real world, there will never be a “perfect dataset” and missing values pose one of the common challenges, undermining the accuracy of predictions and insights. If these are not handled properly, they can skew statistical measures, leading to inaccurate or biased results, and reducing our analysis's effectiveness. Addressing this challenge is crucial for robust analyses and reliable models. Therefore, focusing deeply on dealing with missing values can help us get a much better interpretation of the data, and more precise conclusions and decision-making.

As mentioned, the main goal of the project is to focus on tackling missing values in Australian rainfall data for more accurate and reliable weather forecasts. Each row in the dataset represents a unique observation of rainfall, crucial for understanding patterns and fluctuations across Australia. Therefore, our objective is to predict rainfall amounts based on environmental factors. Through data exploration, data preprocessing methods, comparison of imputation techniques, and evaluation of predictive models, we aim to provide actionable insights for handling missing values effectively. By enhancing the accuracy of rainfall predictions, we aim to improve decision-making and offer insights for weather forecasts and real-world applications in related fields such as agriculture, disaster management, and daily life planning. 	

## Core Methodology, Definitions, and Additional Elements

### The Nature of Missing Values 

Missing data, where values are not recorded for certain variables in observations, is a common issue in research and can significantly impact data analysis (Ribeiro, 2023). Missing data can reduce statistical power, bias parameter estimation, affect sample representativeness, and complicate data analysis, potentially leading to invalid conclusions (Padgett, 2014). Understanding the mechanism behind missing data is crucial because it helps choose appropriate methods for handling missing values and interpreting the results of their analyses. Generally, there are three types of missing data according to the mechanisms of missingness. 

#### Missing Completely at Random (MCAR)
MCAR, or Missing Completely at Random, occurs when the probability of data being missing is completely unrelated to any observed or unobserved variables within the dataset. In simpler terms, the absence of data has no pattern or correlation with any other variables. It is as if the missing data points are randomly selected without any identifiable pattern or underlying reason. However, such occurrences are rare in real-world studies and typically arise from factors like data loss due to equipment failures or transit issues (Kang, 2013).

#### Missing at Random (MAR)
MAR, or Missing at Random, describes a scenario where the likelihood of missing data depends on observed responses rather than unobserved variables. In other words, a systematic relationship exists between the missing values and the observed data, but this relationship can be explained by the variables already present in the dataset. However, the specific values that are missing are still considered random. While more prevalent in real-world studies compared to MCAR, MAR still needs attention as missing data patterns must be addressed and may impact the validity of analyses (Salgado, 2016).

#### Missing Not at Random (MNAR)
MNAR, or Missing Not at Random, occurs when the absence of data is linked to information not included in the dataset. In other words, missingness is influenced by factors beyond those recorded in the dataset, posing a significant challenge as it introduces bias into estimates and can lead to distorted conclusions. MNAR is particularly problematic because it is often challenging to identify and diagnose. Addressing MNAR requires modeling the missing data and integrating it into the analysis framework to mitigate its impact on the validity of results (Jiaxu et al., 2022).

### Models
In each of the datasets, we have applied six different classification algorithms. These algorithms are used to predict outcomes that are either true or false. The goal is to determine which model performs best in terms of accuracy, precision, and other performance measures for each specific dataset. This helps us understand which algorithm is most effective for a given dataset and prediction task. Before proceeding with the analysis and the project, it's essential to grasp the functioning and construction of each model. Understanding each model's mechanics provides insight into how it makes predictions and its underlying assumptions. This comprehension enables us to interpret the results more effectively and choose the most suitable model for our specific dataset and problem. All the classification models have been used from the sklearn library.

#### Logistic Regression Classifier
Logistic regression predicts the likelihood of an event based on independent variables, making it valuable for classification tasks. By transforming odds into probabilities, it generates predictions bounded between 0 and 1. Coefficients are optimized through maximum likelihood estimation, allowing for efficient prediction (IBM, 2022).

#### Decision Tree Classifier
A decision tree is a type of algorithm used in machine learning for tasks like sorting data into categories or making predictions. It's like a flowchart, starting with a main question (the root node) and then branching out based on different answers (branches) to eventually reach conclusions (leaf nodes). It's designed to divide data into smaller, more manageable groups by making decisions at each step. The goal is to create simple, easy-to-understand rules that accurately predict outcomes. Decision trees can get complex as they grow, so techniques like pruning (removing unnecessary branches) and using ensembles (groups of trees) help keep them accurate and efficient (IBM, 2023).

#### Random Forest Classifier
A random forest is a machine-learning algorithm that combines the outputs of multiple decision trees to make predictions. By using a collection of decision trees and injecting randomness into the process, random forests reduce the risk of overfitting and improve accuracy. Each tree in the forest is built on a subset of the data and a subset of features, resulting in a diverse set of trees that work together to provide more accurate predictions (IBM, 2023b).

#### Gradient Boosting Classifier
Gradient boosting is a powerful machine learning technique that combines weak learners, typically decision trees, into a strong predictive model. It operates by sequentially adding trees to correct the errors of the previous ones, using a gradient descent approach to minimize a chosen loss function. This method, marked by its flexibility and ability to handle various types of data, is enhanced through techniques like tree constraints, shrinkage, random sampling, and penalized learning, which mitigates overfitting and enhances predictive accuracy (Jason Brownlee, 2018).

#### KNeighbors Classifier
The K-Nearest Neighbors (KNN) classifier is a type of supervised learning algorithm used for classification tasks. It makes predictions based on the similarity of input data points to the known data points in the training dataset. By creating neighborhoods in the dataset, KNN assigns new data samples to the neighborhoods where they best fit. KNN is particularly effective when dealing with numerical data and a small number of features, and it excels in scenarios with less scattered data and few outliers (Alves, 2021).

#### Adaboost Classier
AdaBoost, short for Adaptive Boosting, is a powerful ensemble learning algorithm that combines multiple weak classifiers to create a strong predictive model. Its main idea involves iteratively training weak classifiers on different subsets of the training data, assigning higher weights to misclassified samples in each iteration. By focusing on challenging examples, AdaBoost enables subsequent classifiers to improve their performance. The algorithm starts by assigning equal weights to all training examples, then iterates through training weak classifiers, adjusting sample weights, and combining classifier predictions based on their performance. This process continues for a specified number of iterations, resulting in a final prediction based on the weighted votes of all weak classifiers (Wizards, 2023).

### Performance Metrics For Evaluating Classification Tasks 
In general, accuracy, F1 score, precision, and recall are commonly used metrics for evaluating model performance on benchmark datasets, particularly in binary classification problems. These metrics are derived from a confusion matrix, which organizes predicted and observed class labels (Blagec et al., 2020).

#### Accuracy
Accuracy measures the proportion of correct predictions out of all observations and applies to both binary and multiclass classifiers. However, its limitation lies in its inability to provide meaningful insights when dealing with imbalanced datasets, where one class dominates the data. This can lead to the "accuracy paradox," where a model predicting only the majority class achieves high accuracy despite not effectively capturing the predictive power of the data.

#### Precision 
Precision measure, also known as positive predictive value, represents the ratio of true positives to the total of true positives and false positives. It indicates the likelihood that a randomly chosen instance predicted as positive is indeed a true positive. A classifier with no false positives achieves a precision score of 1.

#### Recall
Recall, also referred to as sensitivity, measures the proportion of positive instances that are accurately identified as positive by the classifier. It estimates the likelihood that a randomly chosen true positive instance is correctly predicted as positive. Recall focuses on correctly identifying all positive instances, irrespective of how they are classified by the model as positive or negative.

#### F1-Score
The F1 score, derived from the F-measure, represents the harmonic mean of precision and recall, assigning equal importance to both metrics. However, despite its widespread use, concerns have arisen regarding its suitability for evaluating classifiers in machine learning tasks. The F1 score may produce misleading results, particularly when classifiers exhibit a bias toward predicting the majority class. Additionally, its focus on only one class and its insensitivity to the number of true negatives raise further concerns, as does its susceptibility to the swapping of class labels.

#### ROC and AUC
Another set of metrics for evaluating binary classifiers involves Area Under the Curve (AUC) metrics, which analyze the curve generated by comparing two metrics derived from the confusion matrix across all possible decision thresholds. Receiver Operator Characteristic AUC (ROC-AUC, also known as C-statistic or C-index) is the most commonly used AUC metric, representing the trade-off between the true positive rate (recall, sensitivity) and the false positive rate. ROC-AUC can be interpreted as the probability that a randomly chosen positive case has a higher predicted risk than a randomly chosen negative case.

### Strategies and Imputation Techniques for Handling Missing Data
Dealing with missing data can be approached through two distinct strategies. In this project, the first strategy simply disregards missing values such as listwise deletion or pairwise deletion, while the second addresses missing values through imputation methods such as Mean, Median, Mode, Expectation Maximization (EM), Multiple Imputation by Chained Equations (MICE), K Nearest Neighbors (KNN), Regression, and Interpolation. 

## Understanding the Data
The quantitative data utilized for this paper is sourced from a comprehensive dataset available on the Kaggle website titled Rain in Australia (https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package), encompassing approximately 10 years of daily weather observations from numerous locations across Australia. The data itself originally came from the Australian Bureau of Meteorology's Daily Weather Observations. Additional weather metrics for Australia can be found within the bureau's Climate Data Online web app. This dataset contains over 145,000 observations and 23 features, including 6 categorical variables and 17 numerical variables. Notably, the target variable of interest is 'RainTomorrow,' classified as either 'YES' or 'NO,' providing a pivotal focus for predictive modeling and analysis within the study.

Table 1: The feature names of the dataset with their descriptions.

| Field         | Description                                                                    |
|---------------|--------------------------------------------------------------------------------|
| Location      | Name of the city from Australia.                                               |
| MinTemp       | Minimum temperature during a particular day. (degree Celsius)                  |
| MaxTemp       | Maximum temperature during a particular day. (degree Celsius)                  |
| Rainfall      | Rainfall during a particular day. (millimeters)                                |
| Evaporation   | Evaporation during a particular day. (millimeters)                             |
| Sunshine      | Bright sunshine during a particular day. (hours)                               |
| WindGusDir    | Direction of the strongest gust during a particular day. (16 compass points)    |
| WindGuSpeed   | Speed of strongest gust during a particular day. (kilometers per hour)         |
| WindDir9am    | Direction of the wind for 10 min prior to 9 am. (compass points)               |
| WindDir3pm    | Direction of the wind for 10 min prior to 3 pm. (compass points)               |
| WindSpeed9am  | Speed of the wind for 10 min prior to 9 am. (kilometers per hour)              |
| WindSpeed3pm  | Speed of the wind for 10 min prior to 3 pm. (kilometers per hour)              |
| Humidity9am   | Humidity of the wind at 9 am. (percent)                                         |
| Humidity3pm   | Humidity of the wind at 3 pm. (percent)                                         |
| Pressure9am   | Atmospheric pressure at 9 am. (hectopascals)                                    |
| Pressure3pm   | Atmospheric pressure at 3 pm. (hectopascals)                                    |
| Cloud9am      | Cloud-obscured portions of the sky at 9 am. (eighths)                           |
| Cloud3pm      | Cloud-obscured portions of the sky at 3 pm. (eighths)                           |
| Temp9am       | Temperature at 9 am. (degree Celsius)                                           |
| Temp3pm       | Temperature at 3 pm. (degree Celsius)                                           |
| RainToday     | If today is rainy then ‘Yes’. If today is not rainy then ‘No’.                 |
| RainTomorrow  | If tomorrow is rainy then 1 (Yes). If tomorrow is not rainy then 0 (No).       |

## Importing Libraries 
In this section, we are importing the necessary libraries for the entire project, organizing them based on their respective applications. The libraries have been categorized and ordered according to their specific utility in the project.

```{python}
#| label: packages-data

#Importing the needed libraries only in this code chunk

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')
import time
import math

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, accuracy_score, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.utils import resample
from sklearn import metrics
from itertools import cycle
from sklearn.impute import KNNImputer
from sklearn.model_selection import cross_validate
```

## Importing the Data
```{python}
#| label: loading the CSV file
weather = pd.read_csv('Data/weatherAUS.csv', sep=",", header=0, index_col=False)
```

```{python}
#| label: displaying information about the dataset
print("Information about the dataset:")
print(weather.info())
```

# IMPLEMENTATION DETAILS
## PREPROCESSING STEPS
### Dropping Rows with Missing Values
```{python}
#| label: dropping rows with missing values in 'RainToday' and 'RainTomorrow'
weather.dropna(subset=['RainToday', 'RainTomorrow'], inplace=True)
```

### Removing Unnecessary Column(s)
```{python}
#| label: removing unnecesasary columns
column_to_remove = ['Date']  

# Remove the specified columns from the DataFrame
weather.drop(columns=column_to_remove, inplace=True)
```

### Label Encoding for Categorical Variables
```{python}
#| label: label encode for raintomorrow and raintoday
categorical_weather = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Iterate through each categorical column and encode its values
for column in categorical_weather:
    weather[column] = label_encoder.fit_transform(weather[column])

weather
```

### Checking for Data Imbalance
```{python}
#| label: checking for imbalanced data
# Count the number of instances for each class in the 'RainTomorrow' column
class_counts = weather['RainTomorrow'].value_counts()

# Calculate the proportion of each class
class_proportions = class_counts / len(weather)

print("Class Distribution:")
print(class_counts)
print("\nClass Proportions:")
print(class_proportions)
```

It means that our dataset is balanced because the percentage of proportion for the class 0 is 78% and the class 1 is 22%.

### Imputing the Categorical Variables with the Mode (Most Common Value)
```{python}
#| label: imputing the categorical variables with mode
# Replace missing values in categorical columns with the mode
weather['WindGustDir'] = weather['WindGustDir'].fillna(weather['WindGustDir'].mode()[0])
weather['WindDir9am'] = weather['WindDir9am'].fillna(weather['WindDir9am'].mode()[0])
weather['WindDir3pm'] = weather['WindDir3pm'].fillna(weather['WindDir3pm'].mode()[0])

```

## IDENTIFYING AND VISUALIZING THE NATURE OF MISSING VALUES
```{python}
#| label: visualizing the number of NAs
# Checking for missing values
print("\nMissing values in each column:")
print(weather.isnull().sum())

# Visualizing missing data pattern
plt.figure(figsize=(12, 8))
sns.heatmap(weather.isnull(), cmap='viridis', cbar=False)
plt.title('Missing Data Pattern')
plt.show()
```

```{python}
#| label: creating the imputation computational time dataframe
#| eval: false

imputation_comp_time  = pd.DataFrame(columns=['Imputation Technique', 'Computational Time'])
```

To tackle the issue of missing data within our dataset, we employed a variety of imputation techniques and strategies that were allocated to each team member. These techniques are crucial for filling in missing values and ensuring the completeness of our dataset for subsequent analysis. 

## PERFORMING STRATEGY AND IMPUTATION TECHNIQUES

### MEDIAN IMPUTATION TECHNIQUE

This approach involves replacing missing values with the median of the existing values for the same attribute within the respective class in the dataset. This method is used instead of the mean to ensure robustness against outliers, especially in skewed distributions (Hameed & Ali, 2023). To determine which model is the best using the median technique, we can compare the results based on each metric across all models

#### FILLING MISSING VALUES BY MEDIAN
```{python}
#| label: median imputation technique - filling missing values
#| eval: false

# Record starting time
start_time = time.time()

# Example: Filling missing values with median
weather_filled_median = weather.fillna(weather.median())

# Record ending time
end_time = time.time()

# Calculate computational time
imputation_comp_time_this = end_time - start_time

# Append the result to the DataFrame
imputation_comp_time = imputation_comp_time.append(
            {'Imputation Technique': 'Median',
            'Computational Time': imputation_comp_time_this},
            ignore_index=True)

# Checking for missing values
print("\nMissing values in each column:")
print(weather_filled_median.isnull().sum())

# Visualizing missing data pattern
plt.figure(figsize=(12, 8))
sns.heatmap(weather_filled_median.isnull(), cmap='viridis', cbar=False)
plt.title('Missing Data Pattern - After filling with Median')
plt.show()
```

#### PREPROCESSING STEPS BEFORE MODELLING: SPLITTING THE DATA
```{python}
#| label: median imputation technique - preprocessing
#| eval: false

X = weather_filled_median.drop(columns = ['RainTomorrow'])
y = weather_filled_median['RainTomorrow']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the training and testing data separately
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)

# Convert scaled data back to DataFrames
X_train_scaled_weather = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled_weather = pd.DataFrame(X_test_scaled, columns=X.columns)
```

#### MODELING WITH MEDIAN
##### LOGISTIC REGRESSION WITH ALL VARIABLES (MEDIAN)
```{python}
#| label: median imputation technique - logistic regression
#| eval: false
start_time = time.time()

log_model = LogisticRegression()

log_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = log_model.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Creatin a performance scores dataframe
results_weather_median = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC Score', 'Computational Time'])

# Append the metrics to the DataFrame
results_weather_median = results_weather_median.append({
    'Model': 'Logistic Regression Classifier',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_median
```

##### LOGISTIC REGRESSION WITH SCALED DATA (MEDIAN)
```{python}
#| label: median imputation technique - logistic regression scaled
#| eval: false
start_time = time.time()

log_model.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = log_model.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_median = results_weather_median.append({
    'Model': 'Logistic Regression Classifier Scaled',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_median
```


##### DECISION TREE WITH ALL VARIABLES (MEDIAN)
```{python}
#| label: median imputation technique - decision tree
#| eval: false
start_time = time.time()

# Create a Decision Tree Classifier
dt_clf = DecisionTreeClassifier()

# Fit the model to the training data
dt_clf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = dt_clf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_median = results_weather_median.append({
    'Model': 'Decision Tree Classifier',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_median
```

##### DECISION TREE WITH SCALED DATA (MEDIAN)
```{python}
#| label: median imputation technique - decision tree scaled
#| eval: false
start_time = time.time()

# Fit the model to the training data
dt_clf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = dt_clf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time 

# Append the metrics to the DataFrame
results_weather_median = results_weather_median.append({
    'Model': 'Decision Tree Classifier Scaled',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_median
```


##### RANDOM FOREST WITH ALL VARIABLES (MEDIAN)
```{python}
#| label: median imputation technique - random forest
#| eval: false
start_time = time.time()

model_rf = RandomForestClassifier()
model_rf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_median = results_weather_median.append({
    'Model': 'Random Forest Classifier',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_median
```

##### RANDOM FOREST WITH SCALED DATA (MEDIAN)
```{python}
#| label: median imputation technique - random forest scaled
#| eval: false
start_time = time.time()

# Fit the model to the training data
model_rf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_median = results_weather_median.append({
    'Model': 'Random Forest Classifier Scaled',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_median
```


##### GRADIENT BOOSTING WITH ALL VARIABLES (MEDIAN)
```{python}
#| label: median imputation technique - gradient boosting
#| eval: false
start_time = time.time()

# Initialize Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier()

# Fit the model
gb_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_gb = gb_classifier.predict(X_test)

# Calculate accuracy for the Gradient Boosting Classifier
accuracy_gb = accuracy_score(y_test, y_pred_gb)
precision_gb = precision_score(y_test, y_pred_gb)
recall_gb = recall_score(y_test, y_pred_gb)
f1_gb = f1_score(y_test, y_pred_gb)
auc_gb = roc_auc_score(y_test, y_pred_gb)

end_time = time.time()
computational_time = end_time - start_time 

# Append the metrics to the DataFrame
results_weather_median = results_weather_median.append({
    'Model': 'Gradient Boosting Classifier',
    'Accuracy': accuracy_gb,
    'Precision': precision_gb,
    'Recall': recall_gb,
    'F1-score': f1_gb,
    'ROC AUC Score': auc_gb,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_median
```

##### GRADIENT BOOSTING WITH SCALED DATA (MEDIAN)
```{python}
#| label: median imputation technique - gradient boosting scaled
#| eval: false
start_time = time.time()
# Initialize Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier()

# Fit the model
gb_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_gbs = gb_classifier.predict(X_test_scaled_weather)

# Calculate accuracy for the Gradient Boosting Classifier
accuracy_gbs = accuracy_score(y_test, y_pred_gbs)
precision_gbs = precision_score(y_test, y_pred_gbs)
recall_gbs = recall_score(y_test, y_pred_gbs)
f1_gbs = f1_score(y_test, y_pred_gbs)
auc_gbs = roc_auc_score(y_test, y_pred_gbs)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_median = results_weather_median.append({
    'Model': 'Gradient Boosting Classifier Scaled',
    'Accuracy': accuracy_gbs,
    'Precision': precision_gbs,
    'Recall': recall_gbs,
    'F1-score': f1_gbs,
    'ROC AUC Score': auc_gbs,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_median
```


##### KNEIGHBORS WITH ALL VARIABLES (MEDIAN)
```{python}
#| label: median imputation technique - knn 
#| eval: false
X_test = np.array(X_test)

start_time = time.time()

# Initialize K-Nearest Neighbors Classifier
knn_classifier = KNeighborsClassifier()

# Fit the model
knn_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_knn = knn_classifier.predict(X_test)

# Calculate accuracy for the K-Nearest Neighbors Classifier
accuracy_knn = accuracy_score(y_test, y_pred_knn)
precision_knn = precision_score(y_test, y_pred_knn)
recall_knn = recall_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)
auc_knn = roc_auc_score(y_test, y_pred_knn)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_median = results_weather_median.append({
    'Model': 'KNN Classifier',
    'Accuracy': accuracy_knn,
    'Precision': precision_knn,
    'Recall': recall_knn,
    'F1-score': f1_knn,
    'ROC AUC Score': auc_knn,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_median
```

##### KNEIGHBORS WITH SCALED DATA (MEDIAN)
```{python}
#| label: median imputation technique - knn scaled 
#| eval: false
X_test_scaled_weather = np.array(X_test_scaled_weather)

start_time = time.time()

# Fit the model
knn_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_knns = knn_classifier.predict(X_test_scaled_weather)

# Append the accuracy score for the second model to the DataFrame
accuracy_knns = accuracy_score(y_test, y_pred_knns)
precision_knns = precision_score(y_test, y_pred_knns)
recall_knns = recall_score(y_test, y_pred_knns)
f1_knns = f1_score(y_test, y_pred_knns)
auc_knns = roc_auc_score(y_test, y_pred_knns)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_median = results_weather_median.append({
    'Model': 'KNN Classifier Scaled',
    'Accuracy': accuracy_knns,
    'Precision': precision_knns,
    'Recall': recall_knns,
    'F1-score': f1_knns,
    'ROC AUC Score': auc_knns,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_median
```


##### ADABOOST WITH ALL VARIABLES (MEDIAN)
```{python}
#| label: median imputation technique - adaboost
#| eval: false
start_time = time.time()
# Initialize AdaBoost Classifier
adaboost_classifier = AdaBoostClassifier()

# Fit the model
adaboost_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_adaboost = adaboost_classifier.predict(X_test)

# Calculate accuracy for the AdaBoost Classifier
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
precision_adaboost = precision_score(y_test, y_pred_adaboost)
recall_adaboost = recall_score(y_test, y_pred_adaboost)
f1_adaboost = f1_score(y_test, y_pred_adaboost)
auc_adaboost = roc_auc_score(y_test, y_pred_adaboost)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_median = results_weather_median.append({
    'Model': 'AdaBoost Classifier',
    'Accuracy': accuracy_adaboost,
    'Precision': precision_adaboost,
    'Recall': recall_adaboost,
    'F1-score': f1_adaboost,
    'ROC AUC Score': auc_adaboost,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_median
```

##### ADABOOST WITH SCALED DATA (MEDIAN)
```{python}
#| label: median imputation technique - adaboost scaled
#| eval: false
start_time = time.time()

# Fit the model
adaboost_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_adaboost = adaboost_classifier.predict(X_test_scaled_weather)

# Calculate accuracy for the AdaBoost Classifier
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
precision_adaboost = precision_score(y_test, y_pred_adaboost)
recall_adaboost = recall_score(y_test, y_pred_adaboost)
f1_adaboost = f1_score(y_test, y_pred_adaboost)
auc_adaboost = roc_auc_score(y_test, y_pred_adaboost)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_median = results_weather_median.append({
    'Model': 'AdaBoost Classifier Scaled',
    'Accuracy': accuracy_adaboost,
    'Precision': precision_adaboost,
    'Recall': recall_adaboost,
    'F1-score': f1_adaboost,
    'ROC AUC Score': auc_adaboost,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_median
```


#### SHOWING THE MODELS PERFORMANCE
```{python}
#| label: median imputation technique - models performance
#| eval: false
results_weather_median
```

```{python}
#| label: median imputation technique - save model table
#| eval: false
# Save the DataFrame to a CSV file
results_weather_median.to_csv('photoDF/results_weather_median.csv', index=False)
```

Based on the performance metrics after median imputation, **the Random Forest Classifier (Original)** appears to be the best-performing model for rain prediction in Australia, as it achieves the highest values across most evaluation criteria, including accuracy, precision, F1-score, and ROC/AUC, despite it has a relatively high computational time compared to some other models.

### MODE IMPUTATION TECHNIQUE

Mode imputation stands as one of the most straightforward techniques for dealing with missing values in categorical data, where each missing value is replaced with the mode or the most frequent of the available non-missing values of each variable. This approach involves a single imputation, as it replaces each missing observation with a single value. Despite its common use, mode imputation has a significant drawback because it concentrates imputed values around the mode, leading to spikes in the distribution and artificially reducing variance (Torres & Juan, 2014). This method is usually suitable for missing completely at random (MCAR) data (Makaba & Dogo, 2019).

#### FILLING MISSING VALUES BY MODE
```{python}
#| label: mode imputation technique - filling missing values
#| eval: false
# Record starting time
start_time = time.time()

# Example: Filling missing values with mode
weather_filled_mode = weather.fillna(weather.mode().iloc[0])

# Record ending time
end_time = time.time()

# Calculate computational time
imputation_comp_time_this = end_time - start_time

# Append the result to the DataFrame
imputation_comp_time = imputation_comp_time.append(
            {'Imputation Technique': 'Mode',
            'Computational Time': imputation_comp_time_this},
            ignore_index=True)

# Checking for missing values
print("\nMissing values in each column:")
print(weather_filled_mode.isnull().sum())

# Visualizing missing data pattern
plt.figure(figsize=(12, 8))
sns.heatmap(weather_filled_mode.isnull(), cmap='viridis', cbar=False)
plt.title('Missing Data Pattern - After filling with Mode')
plt.show()
```

#### PREPROCESSING STEPS BEFORE MODELLING: SPLITTING THE DATA
```{python}
#| label: mode imputation technique - preprocessing
#| eval: false
X = weather_filled_mode.drop(columns = ['RainTomorrow'])
y = weather_filled_mode['RainTomorrow']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the training and testing data separately
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)

# Convert scaled data back to DataFrames
X_train_scaled_weather = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled_weather = pd.DataFrame(X_test_scaled, columns=X.columns)
```

#### MODELING WITH MODE
##### LOGISTIC REGRESSION WITH ALL VARIABLES (MODE)
```{python}
#| label: mode imputation technique - logistic regression
#| eval: false

# Creatin a performance scores dataframe
results_weather_mode = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC Score', 'Computational Time'])

# Start timing
start_time = time.time()

# Initialize Logistic Regression model
log_model = LogisticRegression()

# Fit the model on the training data
log_model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = log_model.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Logistic Regression with All Variables (Mode)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_mode = pd.concat([results_weather_mode, new_row], ignore_index=True)

results_weather_mode
```

##### LOGISTIC REGRESSION WITH SCALED DATA (MODE)
```{python}
#| label: mode imputation technique - logistic regression scaled
#| eval: false

# Start timing
start_time = time.time()

# Fit the model on the training data
log_model.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = log_model.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Logistic Regression with Scaled Data (Mode)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_mode = pd.concat([results_weather_mode, new_row], ignore_index=True)

results_weather_mode
```


##### DECISION TREE WITH ALL VARIABLES (MODE)
```{python}
#| label: mode imputation technique - decision tree
#| eval: false

# Start timing
start_time = time.time()

# Initialize Decision Tree model
dt_clf = DecisionTreeClassifier()

# Fit the model on the training data
dt_clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = dt_clf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Decision Tree with All Variables (Mode)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_mode = pd.concat([results_weather_mode, new_row], ignore_index=True)

results_weather_mode
```


##### DECISION TREE WITH SCALED DATA (MODE)
```{python}
#| label: mode imputation technique - decision tree scaled
#| eval: false

# Start timing
start_time = time.time()

# Fit the model on the training data
dt_clf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = dt_clf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Decision Tree with Scaled Data (Mode)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_mode = pd.concat([results_weather_mode, new_row], ignore_index=True)

results_weather_mode
```


##### RANDOM FOREST WITH ALL VARIABLES (MODE)
```{python}
#| label: mode imputation technique - random forest
#| eval: false

# Start timing
start_time = time.time()

# Initialize Random Forest model
model_rf = RandomForestClassifier()

# Fit the model on the training data
model_rf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = model_rf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Random Forest with All Variables (Mode)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_mode = pd.concat([results_weather_mode, new_row], ignore_index=True)

results_weather_mode
```


##### RANDOM FOREST WITH SCALED DATA (MODE)

```{python}
#| label: mode imputation technique - random forest scaled
#| eval: false

# Start timing
start_time = time.time()

# Fit the model on the training data
model_rf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Random Forest with Scaled Data (Mode)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_mode = pd.concat([results_weather_mode, new_row], ignore_index=True)

results_weather_mode
```


##### GRADIENT BOOSTING WITH ALL VARIABLES (MODE)
```{python}
#| label: mode imputation technique - gradient boosting
#| eval: false

# Start timing
start_time = time.time()

# Initialize Gradient Boosting model
gb_classifier = GradientBoostingClassifier()

# Fit the model on the training data
gb_classifier.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = gb_classifier.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Gradient Boosting with All Variables (Mode)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_mode = pd.concat([results_weather_mode, new_row], ignore_index=True)

results_weather_mode
```

##### GRADIENT BOOSTING WITH SCALED DATA (MODE)
```{python}
#| label: mode imputation technique - gradient boosting scaled
#| eval: false

# Start timing
start_time = time.time()

# Fit the model on the training data
gb_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = gb_classifier.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Gradient Boosting with Scaled Data (Mode)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_mode = pd.concat([results_weather_mode, new_row], ignore_index=True)

results_weather_mode
```


##### KNEIGHBORS WITH ALL VARIABLES (MODE)
```{python}
#| label: mode imputation technique - KNN
#| eval: false

X_test = np.array(X_test)

# Start timing
start_time = time.time()

# Initialize Logistic Regression model
knn_classifier = KNeighborsClassifier()

# Fit the model on the training data
knn_classifier.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = knn_classifier.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'KNeighbors with All Variables (Mode)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_mode = pd.concat([results_weather_mode, new_row], ignore_index=True)

results_weather_mode
```

##### KNEIGHBORS WITH SCALED DATA (MODE)
```{python}
#| label: mode imputation technique - KNN scaled
#| eval: false

X_test_scaled_weather = np.array(X_test_scaled_weather)
# Start timing
start_time = time.time()

# Fit the model on the training data
knn_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = knn_classifier.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'KNeighbors with Scaled Data (Mode)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_mode = pd.concat([results_weather_mode, new_row], ignore_index=True)

results_weather_mode
```


##### ADABOOST WITH ALL VARIABLES (MODE)
```{python}
#| label: mode imputation technique - adaboost
#| eval: false

# Start timing
start_time = time.time()

# Initialize Adaboost model
adaboost_classifier = AdaBoostClassifier()

# Fit the model on the training data
adaboost_classifier.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = adaboost_classifier.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'AdaBoost with All Variables (Mode)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_mode = pd.concat([results_weather_mode, new_row], ignore_index=True)

results_weather_mode
```

##### ADABOOST WITH SCALED DATA (MODE)
```{python}
#| label: mode imputation technique - adaboost scaled
#| eval: false

# Start timing
start_time = time.time()

# Fit the model on the training data
adaboost_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = adaboost_classifier.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'AdaBoost with Scaled Data (Mode)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_mode = pd.concat([results_weather_mode, new_row], ignore_index=True)

results_weather_mode
```


#### SHOWING THE MODEL PERFORMANCE WITH MODE
```{python}
#| label: mode imputation technique - models performance
#| eval: false

results_weather_mode
```

```{python}
#| label: mode imputation technique - save model table
#| eval: false

# Save the DataFrame to a CSV file
results_weather_mode.to_csv('photoDF/results_weather_mode.csv', index=False)
```

Based on the performance metrics after mode imputation, **the Random Forest Classifier (Scaled)** appears to be the best-performing model for rain prediction in Australia. It achieves the highest accuracy, precision, F1-score, and ROC AUC score among all models, despite it has a relatively high computational time compared to some other models.

### MEAN IMPUTATION TECHNIQUE

The mean imputation technique consists of replacing the missing data for a given variable by calculating the mean of all known values of that variable. This method is easy to apply, readily available in most statistical software packages, and faster compared to other techniques. While it yields accurate results for small datasets, it may not be suitable for large datasets (Hameed & Ali, 2023). This method is usually suitable for missing completely at random (MCAR) data (Makaba & Dogo, 2019).

#### FILLING MISSING VALUES BY MEAN

```{python}
#| label: mean imputation technique - filling missing values
#| eval: false

# Record starting time
start_time = time.time()

# Handling missing values using different imputation techniques
# Example: Filling missing values with mean
weather_filled_mean = weather.fillna(weather.mean())

# Record ending time
end_time = time.time()

# Calculate computational time
imputation_comp_time_this = end_time - start_time

# Append the result to the DataFrame
imputation_comp_time = imputation_comp_time.append(
            {'Imputation Technique': 'Mean',
            'Computational Time': imputation_comp_time_this},
            ignore_index=True)

# Checking for missing values
print("\nMissing values in each column:")
print(weather_filled_mean.isnull().sum())

# Visualizing missing data pattern
plt.figure(figsize=(12, 8))
sns.heatmap(weather_filled_mean.isnull(), cmap='viridis', cbar=False)
plt.title('Missing Data Pattern')
plt.show()
```

#### PREPROCESSING STEPS BEFORE MODELLING: SPLITTING THE DATA
```{python}
#| label: mean imputation technique - preprocessing
#| eval: false

X = weather_filled_mean.drop(columns = ['RainTomorrow'])
y = weather_filled_mean['RainTomorrow']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the training and testing data separately
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)

# Convert scaled data back to DataFrames
X_train_scaled_weather = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled_weather = pd.DataFrame(X_test_scaled, columns=X.columns)
```


#### MODELING WITH MEAN
##### LOGISTIC REGRESSION WITH ALL VARIABLES (MEAN)

```{python}
#| label: mean imputation technique - logistic regression
#| eval: false

start_time = time.time()

log_model = LogisticRegression()

log_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = log_model.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Creatin a performance scores dataframe
results_weather_mean = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC Score', 'Computational Time'])

# Append the metrics to the DataFrame
results_weather_mean = results_weather_mean.append({
    'Model': 'Logistic Regression Classifier Mean',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_mean
```

##### LOGISTIC REGRESSION WITH SCALED DATA (MEAN)

```{python}
#| label: mean imputation technique - logistic regression scaled
#| eval: false

start_time = time.time()

log_model.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = log_model.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mean = results_weather_mean.append({
    'Model': 'Logistic Regression Classifier Scaled Mean',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_mean
```



##### DECISION TREE WITH ALL VARIABLES (MEAN)

```{python}
#| label: mean imputation technique - decision tree
#| eval: false

start_time = time.time()

# Create a Decision Tree Classifier
dt_clf = DecisionTreeClassifier()

# Fit the model to the training data
dt_clf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = dt_clf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mean = results_weather_mean.append({
    'Model': 'Decision Tree Classifier Mean',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_mean
```

##### DECISION TREE WITH SCALED DATA (MEAN)
```{python}
#| label: mean imputation technique - decision tree scaled
#| eval: false

start_time = time.time()

# Fit the model to the training data
dt_clf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = dt_clf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time 

# Append the metrics to the DataFrame
results_weather_mean = results_weather_mean.append({
    'Model': 'Decision Tree Classifier Scaled Mean',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_mean
```


##### RANDOM FOREST WITH ALL VARIABLES (MEAN)
```{python}
#| label: mean imputation technique - random forest
#| eval: false

start_time = time.time()

model_rf = RandomForestClassifier()
model_rf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mean = results_weather_mean.append({
    'Model': 'Random Forest Classifier Mean',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_mean
```

##### RANDOM FOREST WITH SCALED DATA (MEAN)
```{python}
#| label: mean imputation technique - random forest scaled
#| eval: false

start_time = time.time()

# Fit the model to the training data
model_rf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mean = results_weather_mean.append({
    'Model': 'Random Forest Classifier Scaled Mean',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_mean
```


##### GRADIENT BOOSTING WITH ALL VARIABLES (MEAN)
```{python}
#| label: mean imputation technique - gradient boosting
#| eval: false

start_time = time.time()

# Initialize Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier()

# Fit the model
gb_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_gb = gb_classifier.predict(X_test)

# Calculate accuracy for the Gradient Boosting Classifier
accuracy_gb = accuracy_score(y_test, y_pred_gb)
precision_gb = precision_score(y_test, y_pred_gb)
recall_gb = recall_score(y_test, y_pred_gb)
f1_gb = f1_score(y_test, y_pred_gb)
auc_gb = roc_auc_score(y_test, y_pred_gb)

end_time = time.time()
computational_time = end_time - start_time 

# Append the metrics to the DataFrame
results_weather_mean = results_weather_mean.append({
    'Model': 'Gradient Boosting Classifier',
    'Accuracy': accuracy_gb,
    'Precision': precision_gb,
    'Recall': recall_gb,
    'F1-score': f1_gb,
    'ROC AUC Score': auc_gb,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_mean
```

##### GRADIENT BOOSTING WITH SCALED DATA (MEAN)
```{python}
#| label: mean imputation technique - gradient boosting scaled
#| eval: false

start_time = time.time()
# Initialize Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier()

# Fit the model
gb_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_gbs = gb_classifier.predict(X_test_scaled_weather)

# Calculate accuracy for the Gradient Boosting Classifier
accuracy_gbs = accuracy_score(y_test, y_pred_gbs)
precision_gbs = precision_score(y_test, y_pred_gbs)
recall_gbs = recall_score(y_test, y_pred_gbs)
f1_gbs = f1_score(y_test, y_pred_gbs)
auc_gbs = roc_auc_score(y_test, y_pred_gbs)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mean = results_weather_mean.append({
    'Model': 'Gradient Boosting Classifier Scaled Mean',
    'Accuracy': accuracy_gbs,
    'Precision': precision_gbs,
    'Recall': recall_gbs,
    'F1-score': f1_gbs,
    'ROC AUC Score': auc_gbs,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_mean
```


##### KNEIGHBORS WITH ALL VARIABLES (MEAN)
```{python}
#| label: mean imputation technique - KNN
#| eval: false

X_test = np.array(X_test)

start_time = time.time()

# Initialize K-Nearest Neighbors Classifier
knn_classifier = KNeighborsClassifier()

# Fit the model
knn_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_knn = knn_classifier.predict(X_test)

# Calculate accuracy for the K-Nearest Neighbors Classifier
accuracy_knn = accuracy_score(y_test, y_pred_knn)
precision_knn = precision_score(y_test, y_pred_knn)
recall_knn = recall_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)
auc_knn = roc_auc_score(y_test, y_pred_knn)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mean = results_weather_mean.append({
    'Model': 'KNN Classifier Mean',
    'Accuracy': accuracy_knn,
    'Precision': precision_knn,
    'Recall': recall_knn,
    'F1-score': f1_knn,
    'ROC AUC Score': auc_knn,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_mean
```

##### KNEIGHBORS WITH SCALED DATA (MEAN)
```{python}
#| label: mean imputation technique - KNN scaled
#| eval: false

X_test_scaled_weather = np.array(X_test_scaled_weather)

start_time = time.time()

# Fit the model
knn_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_knns = knn_classifier.predict(X_test_scaled_weather)

# Append the accuracy score for the second model to the DataFrame
accuracy_knns = accuracy_score(y_test, y_pred_knns)
precision_knns = precision_score(y_test, y_pred_knns)
recall_knns = recall_score(y_test, y_pred_knns)
f1_knns = f1_score(y_test, y_pred_knns)
auc_knns = roc_auc_score(y_test, y_pred_knns)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mean = results_weather_mean.append({
    'Model': 'KNN Classifier Scaled Mean',
    'Accuracy': accuracy_knns,
    'Precision': precision_knns,
    'Recall': recall_knns,
    'F1-score': f1_knns,
    'ROC AUC Score': auc_knns,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_mean
```


##### ADABOOST WITH ALL VARIABLES (MEAN)
```{python}
#| label: mean imputation technique - adaboost
#| eval: false

start_time = time.time()
# Initialize AdaBoost Classifier
adaboost_classifier = AdaBoostClassifier()

# Fit the model
adaboost_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_adaboost = adaboost_classifier.predict(X_test)

# Calculate accuracy for the AdaBoost Classifier
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
precision_adaboost = precision_score(y_test, y_pred_adaboost)
recall_adaboost = recall_score(y_test, y_pred_adaboost)
f1_adaboost = f1_score(y_test, y_pred_adaboost)
auc_adaboost = roc_auc_score(y_test, y_pred_adaboost)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mean = results_weather_mean.append({
    'Model': 'AdaBoost Classifier Mean',
    'Accuracy': accuracy_adaboost,
    'Precision': precision_adaboost,
    'Recall': recall_adaboost,
    'F1-score': f1_adaboost,
    'ROC AUC Score': auc_adaboost,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_mean
```

##### ADABOOST WITH SCALED DATA (MEAN)
```{python}
#| label: mean imputation technique - adaboost scaled
#| eval: false

start_time = time.time()

# Fit the model
adaboost_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_adaboost = adaboost_classifier.predict(X_test_scaled_weather)

# Calculate accuracy for the AdaBoost Classifier
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
precision_adaboost = precision_score(y_test, y_pred_adaboost)
recall_adaboost = recall_score(y_test, y_pred_adaboost)
f1_adaboost = f1_score(y_test, y_pred_adaboost)
auc_adaboost = roc_auc_score(y_test, y_pred_adaboost)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mean = results_weather_mean.append({
    'Model': 'AdaBoost Classifier Scaled Mean',
    'Accuracy': accuracy_adaboost,
    'Precision': precision_adaboost,
    'Recall': recall_adaboost,
    'F1-score': f1_adaboost,
    'ROC AUC Score': auc_adaboost,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_mean
```


#### SHOWING THE MODEL PERFORMANCE

```{python}
#| label: mean imputation technique - models performance
#| eval: false

results_weather_mean
```

```{python}
#| label: mean imputation technique - save model table
#| eval: false

# Save the DataFrame to a CSV file
results_weather_mean.to_csv('photoDF/results_weather_mean.csv', index=False)
```

Based on the performance metrics after mean imputation, **the Random Forest Classifier (Original)** appears to be the best-performing model for rain prediction in Australia. It achieves the highest accuracy, precision, F1-score, and ROC AUC score among all models, despite it has a relatively high computational time compared to some other models.

### MICE IMPUTATION TECHNIQUE

MICE, or Multiple Imputation by Chained Equations, is a specific technique for handling missing data. Typically, MICE operates under the assumption that data is Missing At Random (MAR), meaning that the probability of a value being missing depends only on observed values (Makaba & Dogo, 2019). Initially, this technique iteratively predicts missing values from other variables in the dataset, generating multiple imputed values using regression models. Each missing variable is treated as dependent, with other data as independent variables. It is essential to note that implementing MICE with data that do not meet this assumption may lead to biased estimates (Azur et al., 2011). 

#### FILLING MISSING VALUES BY MICE
```{python}
#| label: mice imputation technique - filling missing values
#| eval: false

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# Record starting time
start_time = time.time()

# Initialize the IterativeImputer with appropriate parameters
imputer = IterativeImputer(max_iter=10, random_state=0)

# Fit the imputer to the data and transform the dataset
weather_filled_mice = pd.DataFrame(imputer.fit_transform(weather), columns=weather.columns)

# Record ending time
end_time = time.time()

# Calculate computational time
imputation_comp_time_this = end_time - start_time

# Append the result to the DataFrame
imputation_comp_time = imputation_comp_time.append(
            {'Imputation Technique': 'Mice',
            'Computational Time': imputation_comp_time_this},
            ignore_index=True)

# Checking for missing values
print("\nMissing values in each column after MICE imputation:")
print(weather_filled_mice.isnull().sum())

# Visualizing missing data pattern
plt.figure(figsize=(12, 8))
sns.heatmap(weather_filled_mice.isnull(), cmap='viridis', cbar=False)
plt.title('Missing Data Pattern after MICE Imputation')
plt.show()
```

#### PREPROCESSING STEPS BEFORE MODELLING: SPLITTING THE DATA 
```{python}
#| label: mice imputation technique - preprocessing
#| eval: false

X = weather_filled_mice.drop(columns = ['RainTomorrow'])
y = weather_filled_mice['RainTomorrow']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the training and testing data separately
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)

# Convert scaled data back to DataFrames
X_train_scaled_weather = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled_weather = pd.DataFrame(X_test_scaled, columns=X.columns)
```

#### MODELING WITH MICE

##### LOGISTIC REGRESSION WITH ALL VARIABLES (MICE)
```{python}
#| label: mice imputation technique - logistic regression
#| eval: false

start_time = time.time()

log_model = LogisticRegression()

log_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = log_model.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Creatin a performance scores dataframe
results_weather_mice = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC Score', 'Computational Time'])

# Append the metrics to the DataFrame
results_weather_mice = results_weather_mice.append({
    'Model': 'Logistic Regression Classifier',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_mice
```

##### LOGISTIC REGRESSION WITH SCALED DATA (MICE)
```{python}
#| label: mice imputation technique - logistic regression scaled
#| eval: false

start_time = time.time()

log_model.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = log_model.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mice = results_weather_mice.append({
    'Model': 'Logistic Regression Classifier Scaled',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_mice
```


##### DECISION TREE WITH ALL VARIABLES (MICE)
```{python}
#| label: mice imputation technique - decision tree
#| eval: false

start_time = time.time()

# Create a Decision Tree Classifier
dt_clf = DecisionTreeClassifier()

# Fit the model to the training data
dt_clf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = dt_clf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mice = results_weather_mice.append({
    'Model': 'Decision Tree Classifier',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_mice
```

##### DECISION TREE WITH SCALED DATA (MICE)
```{python}
#| label: mice imputation technique - decision tree scaled
#| eval: false

start_time = time.time()

# Fit the model to the training data
dt_clf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = dt_clf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time 

# Append the metrics to the DataFrame
results_weather_mice = results_weather_mice.append({
    'Model': 'Decision Tree Classifier Scaled',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_mice
```


##### RANDOM FOREST WITH ALL VARIABLES (MICE)
```{python}
#| label: mice imputation technique - random forest
#| eval: false

start_time = time.time()

model_rf = RandomForestClassifier()
model_rf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mice = results_weather_mice.append({
    'Model': 'Random Forest Classifier',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_mice
```

##### RANDOM FOREST WITH SCALED DATA (MICE)
```{python}
#| label: mice imputation technique - random forest scaled
#| eval: false

start_time = time.time()

# Fit the model to the training data
model_rf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mice = results_weather_mice.append({
    'Model': 'Random Forest Classifier Scaled',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_mice
```


##### GRADIENT BOOSTING WITH ALL VARIABLES (MICE)
```{python}
#| label: mice imputation technique - gradient boosting
#| eval: false

start_time = time.time()

# Initialize Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier()

# Fit the model
gb_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_gb = gb_classifier.predict(X_test)

# Calculate accuracy for the Gradient Boosting Classifier
accuracy_gb = accuracy_score(y_test, y_pred_gb)
precision_gb = precision_score(y_test, y_pred_gb)
recall_gb = recall_score(y_test, y_pred_gb)
f1_gb = f1_score(y_test, y_pred_gb)
auc_gb = roc_auc_score(y_test, y_pred_gb)

end_time = time.time()
computational_time = end_time - start_time 

# Append the metrics to the DataFrame
results_weather_mice = results_weather_mice.append({
    'Model': 'Gradient Boosting Classifier',
    'Accuracy': accuracy_gb,
    'Precision': precision_gb,
    'Recall': recall_gb,
    'F1-score': f1_gb,
    'ROC AUC Score': auc_gb,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_mice
```

##### GRADIENT BOOSTING WITH SCALED DATA (MICE)
```{python}
#| label: mice imputation technique - gradient boosting scaled
#| eval: false

start_time = time.time()
# Initialize Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier()

# Fit the model
gb_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_gbs = gb_classifier.predict(X_test_scaled_weather)

# Calculate accuracy for the Gradient Boosting Classifier
accuracy_gbs = accuracy_score(y_test, y_pred_gbs)
precision_gbs = precision_score(y_test, y_pred_gbs)
recall_gbs = recall_score(y_test, y_pred_gbs)
f1_gbs = f1_score(y_test, y_pred_gbs)
auc_gbs = roc_auc_score(y_test, y_pred_gbs)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mice = results_weather_mice.append({
    'Model': 'Gradient Boosting Classifier Scaled',
    'Accuracy': accuracy_gbs,
    'Precision': precision_gbs,
    'Recall': recall_gbs,
    'F1-score': f1_gbs,
    'ROC AUC Score': auc_gbs,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_mice
```


##### KNEIGHBORS WITH ALL VARIABLES (MICE)
```{python}
#| label: mice imputation technique - KNN
#| eval: false

X_test = np.array(X_test)

start_time = time.time()

# Initialize K-Nearest Neighbors Classifier
knn_classifier = KNeighborsClassifier()

# Fit the model
knn_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_knn = knn_classifier.predict(X_test)

# Calculate accuracy for the K-Nearest Neighbors Classifier
accuracy_knn = accuracy_score(y_test, y_pred_knn)
precision_knn = precision_score(y_test, y_pred_knn)
recall_knn = recall_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)
auc_knn = roc_auc_score(y_test, y_pred_knn)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mice = results_weather_mice.append({
    'Model': 'KNN Classifier',
    'Accuracy': accuracy_knn,
    'Precision': precision_knn,
    'Recall': recall_knn,
    'F1-score': f1_knn,
    'ROC AUC Score': auc_knn,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_mice
```

##### KNEIGHBORS WITH SCALED DATA (MICE)
```{python}
#| label: mice imputation technique - KNN scaled
#| eval: false

X_test_scaled_weather = np.array(X_test_scaled_weather)

start_time = time.time()

# Fit the model
knn_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_knns = knn_classifier.predict(X_test_scaled_weather)

# Append the accuracy score for the second model to the DataFrame
accuracy_knns = accuracy_score(y_test, y_pred_knns)
precision_knns = precision_score(y_test, y_pred_knns)
recall_knns = recall_score(y_test, y_pred_knns)
f1_knns = f1_score(y_test, y_pred_knns)
auc_knns = roc_auc_score(y_test, y_pred_knns)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mice = results_weather_mice.append({
    'Model': 'KNN Classifier Scaled',
    'Accuracy': accuracy_knns,
    'Precision': precision_knns,
    'Recall': recall_knns,
    'F1-score': f1_knns,
    'ROC AUC Score': auc_knns,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_mice
```


##### ADABOOST WITH ALL VARIABLES (MICE)
```{python}
#| label: mice imputation technique - adaboost
#| eval: false

start_time = time.time()
# Initialize AdaBoost Classifier
adaboost_classifier = AdaBoostClassifier()

# Fit the model
adaboost_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_adaboost = adaboost_classifier.predict(X_test)

# Calculate accuracy for the AdaBoost Classifier
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
precision_adaboost = precision_score(y_test, y_pred_adaboost)
recall_adaboost = recall_score(y_test, y_pred_adaboost)
f1_adaboost = f1_score(y_test, y_pred_adaboost)
auc_adaboost = roc_auc_score(y_test, y_pred_adaboost)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mice = results_weather_mice.append({
    'Model': 'AdaBoost Classifier',
    'Accuracy': accuracy_adaboost,
    'Precision': precision_adaboost,
    'Recall': recall_adaboost,
    'F1-score': f1_adaboost,
    'ROC AUC Score': auc_adaboost,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_mice
```

##### ADABOOST WITH SCALED DATA (MICE)
```{python}
#| label: mice imputation technique - adaboost scaled
#| eval: false

start_time = time.time()

# Fit the model
adaboost_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_adaboost = adaboost_classifier.predict(X_test_scaled_weather)

# Calculate accuracy for the AdaBoost Classifier
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
precision_adaboost = precision_score(y_test, y_pred_adaboost)
recall_adaboost = recall_score(y_test, y_pred_adaboost)
f1_adaboost = f1_score(y_test, y_pred_adaboost)
auc_adaboost = roc_auc_score(y_test, y_pred_adaboost)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_mice = results_weather_mice.append({
    'Model': 'AdaBoost Classifier Scaled',
    'Accuracy': accuracy_adaboost,
    'Precision': precision_adaboost,
    'Recall': recall_adaboost,
    'F1-score': f1_adaboost,
    'ROC AUC Score': auc_adaboost,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_mice
```

#### SHOWING THE MODEL PERFORMANCE
```{python}
#| label: mice imputation technique - models performance
#| eval: false

results_weather_mice
```

```{python}
#| label: mice imputation technique - save model table
#| eval: false

# Save the DataFrame to a CSV file
results_weather_mice.to_csv('photoDF/results_weather_mice.csv', index=False)
```

Based on the performance metrics after MICE (Multiple Imputation by Chained Equations) imputation, the Random Forest Classifier (Scaled) appears to be the best-performing model for rain prediction in Australia. It achieves the highest accuracy, precision, F1-score, and ROC AUC score among all models, despite it has a relatively high computational time compared to some other models.

### KNN IMPUTATION TECHNIQUE

The KNN imputation method involves replacing missing values with similar ones based on the values of the nearest neighbors or some distance metrics such as Euclidean distance (Hameed & Ali, 2023). The effectiveness of this method relies heavily on the dataset size and finding the right value for k. Typically, k-NN assumes that data are missing completely at random (MCAR) (Makaba & Dogo, 2019). It is advantageous for datasets with both qualitative and quantitative attributes, as it does not require predictive models for each attribute with missing data. Additionally, it can handle instances with multiple missing values and considers data correlation. However, the algorithm's search through the entire dataset can be computationally intensive (Gabr et al., 2023).

#### FILLING MISSING VALUES BY KNN
```{python}
#| label: knn imputation technique - filling missing values
#| eval: false

# Record starting time
start_time = time.time()

# Instantiate the KNNImputer
imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')

# Fit the imputer to your data and transform it
weather_filled_knn = imputer.fit_transform(weather)

# Convert the array back to a DataFrame if necessary
weather_filled_knn = pd.DataFrame(weather_filled_knn, columns=weather.columns)

# Record ending time
end_time = time.time()

# Calculate computational time
imputation_comp_time_this = end_time - start_time

# Append the result to the DataFrame
imputation_comp_time = imputation_comp_time.append(
            {'Imputation Technique': 'KNN',
            'Computational Time': imputation_comp_time_this},
            ignore_index=True)

# Checking for missing values
print("\nMissing values in each column:")
print(weather_filled_knn.isnull().sum())

# Visualizing missing data pattern
plt.figure(figsize=(12, 8))
sns.heatmap(weather_filled_knn.isnull(), cmap='viridis', cbar=False)
plt.title('Missing Data Pattern')
plt.show()
```

#### PREPROCESSING STEPS BEFORE MODELLING: SPLITTING THE DATA
```{python}
#| label: knn imputation technique - preprocessing
#| eval: false

X = weather_filled_knn.drop(columns = ['RainTomorrow'])
y = weather_filled_knn['RainTomorrow']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the training and testing data separately
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)

# Convert scaled data back to DataFrames
X_train_scaled_weather = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled_weather = pd.DataFrame(X_test_scaled, columns=X.columns)
```


#### MODELING WITH KNN
##### LOGISTIC REGRESSION WITH ALL VARIABLES (KNN)
```{python}
#| label: knn imputation technique - logistic regression
#| eval: false

# Creatin a performance scores dataframe
results_weather_knn = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC Score', 'Computational Time'])

# Start timing
start_time = time.time()

# Initialize Logistic Regression model
log_model = LogisticRegression()

# Fit the model on the training data
log_model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = log_model.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Logistic Regression with All Variables (KNN)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_knn = pd.concat([results_weather_knn, new_row], ignore_index=True)

results_weather_knn
```

##### LOGISTIC REGRESSION WITH SCALED DATA (KNN)
```{python}
#| label: knn imputation technique - logistic regression scaled
#| eval: false

# Start timing
start_time = time.time()

# Fit the model on the training data
log_model.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = log_model.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Logistic Regression with Scaled Data (KNN)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_knn = pd.concat([results_weather_knn, new_row], ignore_index=True)

results_weather_knn
```


##### DECISION TREE WITH ALL VARIABLES (KNN)
```{python}
#| label: knn imputation technique - decision tree
#| eval: false

# Start timing
start_time = time.time()

# Initialize Decision Tree model
dt_clf = DecisionTreeClassifier()

# Fit the model on the training data
dt_clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = dt_clf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Decision Tree with All Variables (KNN)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_knn = pd.concat([results_weather_knn, new_row], ignore_index=True)

results_weather_knn
```

##### DECISION TREE WITH SCALED DATA (KNN)
```{python}
#| label: knn imputation technique - decision tree scaled
#| eval: false

# Start timing
start_time = time.time()

# Fit the model on the training data
dt_clf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = dt_clf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Decision Tree with Scaled Data (KNN)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_knn = pd.concat([results_weather_knn, new_row], ignore_index=True)

results_weather_knn
```


##### RANDOM FOREST WITH ALL VARIABLES (KNN)
```{python}
#| label: knn imputation technique - random forest
#| eval: false

# Start timing
start_time = time.time()

# Initialize Random Forest model
model_rf = RandomForestClassifier()

# Fit the model on the training data
model_rf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = model_rf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Random Forest with All Variables (KNN)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_knn = pd.concat([results_weather_knn, new_row], ignore_index=True)

results_weather_knn
```

##### RANDOM FOREST WITH SCALED DATA (KNN)
```{python}
#| label: knn imputation technique - random forest scaled
#| eval: false

# Start timing
start_time = time.time()

# Fit the model on the training data
model_rf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Random Forest with Scaled Data (KNN)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_knn = pd.concat([results_weather_knn, new_row], ignore_index=True)

results_weather_knn
```


##### GRADIENT BOOSTING WITH ALL VARIABLES (KNN)
```{python}
#| label: knn imputation technique - gradient boosting
#| eval: false

# Start timing
start_time = time.time()

# Initialize Gradient Boosting model
gb_classifier = GradientBoostingClassifier()

# Fit the model on the training data
gb_classifier.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = gb_classifier.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Gradient Boosting with All Variables (KNN)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_knn = pd.concat([results_weather_knn, new_row], ignore_index=True)

results_weather_knn
```

##### GRADIENT BOOSTING WITH SCALED DATA (KNN)
```{python}
#| label: knn imputation technique - gradient boosting scaled
#| eval: false

# Start timing
start_time = time.time()

# Fit the model on the training data
gb_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = gb_classifier.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Gradient Boosting with Scaled Data (KNN)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_knn = pd.concat([results_weather_knn, new_row], ignore_index=True)

results_weather_knn
```


##### KNEIGHBORS WITH ALL VARIABLES (KNN)
```{python}
#| label: knn imputation technique - knn
#| eval: false

X_test = np.array(X_test)
# Start timing
start_time = time.time()

# Initialize Logistic Regression model
knn_classifier = KNeighborsClassifier()

# Fit the model on the training data
knn_classifier.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = knn_classifier.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'KNeighbors with All Variables (KNN)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_knn = pd.concat([results_weather_knn, new_row], ignore_index=True)

results_weather_knn
```

##### KNEIGHBORS WITH SCALED DATA (KNN)
```{python}
#| label: knn imputation technique - knn scaled
#| eval: false

X_test_scaled_weather = np.array(X_test_scaled_weather)
# Start timing
start_time = time.time()

# Fit the model on the training data
knn_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = knn_classifier.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'KNeighbors with Scaled Data (KNN)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_knn = pd.concat([results_weather_knn, new_row], ignore_index=True)

results_weather_knn
```


##### ADABOOST WITH ALL VARIABLES (KNN)
```{python}
#| label: knn imputation technique - adaboost
#| eval: false

# Start timing
start_time = time.time()

# Initialize Logistic Regression model
adaboost_classifier = AdaBoostClassifier()

# Fit the model on the training data
adaboost_classifier.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = adaboost_classifier.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'AdaBoost with All Variables (KNN)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_knn = pd.concat([results_weather_knn, new_row], ignore_index=True)

results_weather_knn
```

##### ADABOOST WITH SCALED DATA (KNN)
```{python}
#| label: knn imputation technique - adaboost scaled
#| eval: false

# Start timing
start_time = time.time()

# Fit the model on the training data
adaboost_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = adaboost_classifier.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'AdaBoost with Scaled Data (KNN)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_knn = pd.concat([results_weather_knn, new_row], ignore_index=True)

results_weather_knn
```

#### SHOWING THE MODEL PERFORMANCE WITH KNN
```{python}
#| label: knn imputation technique - models performance
#| eval: false

results_weather_knn
```

```{python}
#| label: knn imputation technique - save model table
#| eval: false

# Save the DataFrame to a CSV file
results_weather_knn.to_csv('photoDF/results_weather_knn.csv', index=False)
```

Based on the performance metrics after KNN (K-Nearest Neighbors) imputation, **the Random Forest Classifier (Scaled)** appears to be the best-performing model for rain prediction in Australia, as it achieves the highest values across most evaluation criteria, including accuracy, precision, F1-score, and ROC/AUC, despite it has a relatively high computational time compared to some other models.

### EM IMPUTATION TECHNIQUE

The Expectation-Maximization (EM) method is a maximum likelihood approach used to impute missing values in datasets. It estimates parameters using available data, creates regression equations to predict missing values, and iterates the process until convergence (Song & Shepperd, 2007). Typically, EM assumes that data are missing at random (MAR) (Makaba & Dogo, 2019). However, it may take time to converge, especially with large missing data fractions, and can lead to biased parameter estimates and underestimate standard errors. Single imputation with EM may overestimate precision due to the underestimation of standard errors (Kang, 2013).

#### FILLING MISSING VALUES BY EM
```{python}
#| label: em imputation technique - filling missing values
#| eval: false

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# Record starting time
start_time = time.time()

# Create an instance of IterativeImputer with the EM algorithm
em_imputer = IterativeImputer(max_iter=10, random_state=0)

# Impute missing values using the EM algorithm
weather_filled_em = em_imputer.fit_transform(weather)

weather_filled_em = pd.DataFrame(weather_filled_em, columns=weather.columns)

# Record ending time
end_time = time.time()

# Calculate computational time
imputation_comp_time_this = end_time - start_time

# Append the result to the DataFrame
imputation_comp_time = imputation_comp_time.append(
            {'Imputation Technique': 'EM',
            'Computational Time': imputation_comp_time_this},
            ignore_index=True)

# Checking for missing values
print("\nMissing values in each column:")
print(weather_filled_em.isnull().sum())

# Visualizing missing data pattern
plt.figure(figsize=(12, 8))
sns.heatmap(weather_filled_em.isnull(), cmap='viridis', cbar=False)
plt.title('Missing Data Pattern')
plt.show()

```


#### PREPROCESSING STEPS BEFORE MODELLING: SPLITTING THE DATA

```{python}
#| label: em imputation technique - preprocessing
#| eval: false

X = weather_filled_em.drop(columns = ['RainTomorrow'])
y = weather_filled_em['RainTomorrow']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the training and testing data separately
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)

# Convert scaled data back to DataFrames
X_train_scaled_weather = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled_weather = pd.DataFrame(X_test_scaled, columns=X.columns)
```


#### MODELING WITH EM

##### LOGISTIC REGRESSION WITH ALL VARIABLES (EM)

```{python}
#| label: em imputation technique - logistic regression
#| eval: false

start_time = time.time()

log_model = LogisticRegression()

log_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = log_model.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Creatin a performance scores dataframe
results_weather_em = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC Score', 'Computational Time'])

# Append the metrics to the DataFrame
results_weather_em = results_weather_em.append({
    'Model': 'Logistic Regression Classifier EM',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_em
```

##### LOGISTIC REGRESSION WITH SCALED DATA (EM)

```{python}
#| label: em imputation technique - logistic regression scaled
#| eval: false
#| 
start_time = time.time()

log_model.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = log_model.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_em = results_weather_em.append({
    'Model': 'Logistic Regression Classifier Scaled EM',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_em
```


##### DECISION TREE WITH ALL VARIABLES (EM)

```{python}
#| label: em imputation technique - decision tree
#| eval: false

start_time = time.time()

# Create a Decision Tree Classifier
dt_clf = DecisionTreeClassifier()

# Fit the model to the training data
dt_clf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = dt_clf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_em = results_weather_em.append({
    'Model': 'Decision Tree Classifier EM',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_em
```

##### DECISION TREE WITH SCALED DATA (EM)
```{python}
#| label: em imputation technique - decision tree scaled
#| eval: false

start_time = time.time()

# Fit the model to the training data
dt_clf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = dt_clf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time 

# Append the metrics to the DataFrame
results_weather_em = results_weather_em.append({
    'Model': 'Decision Tree Classifier Scaled EM',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_em
```


##### RANDOM FOREST WITH ALL VARIABLES (EM)
```{python}
#| label: em imputation technique - random forest
#| eval: false

start_time = time.time()

model_rf = RandomForestClassifier()
model_rf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_em = results_weather_em.append({
    'Model': 'Random Forest Classifier EM',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_em
```

##### RANDOM FOREST WITH SCALED DATA (EM)
```{python}
#| label: em imputation technique - random forest scaled
#| eval: false

start_time = time.time()

# Fit the model to the training data
model_rf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_em = results_weather_em.append({
    'Model': 'Random Forest Classifier Scaled Mean',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_em
```


##### GRADIENT BOOSTING WITH ALL VARIABLES (EM)
```{python}
#| label: em imputation technique - gradient boosting
#| eval: false

start_time = time.time()

# Initialize Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier()

# Fit the model
gb_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_gb = gb_classifier.predict(X_test)

# Calculate accuracy for the Gradient Boosting Classifier
accuracy_gb = accuracy_score(y_test, y_pred_gb)
precision_gb = precision_score(y_test, y_pred_gb)
recall_gb = recall_score(y_test, y_pred_gb)
f1_gb = f1_score(y_test, y_pred_gb)
auc_gb = roc_auc_score(y_test, y_pred_gb)

end_time = time.time()
computational_time = end_time - start_time 

# Append the metrics to the DataFrame
results_weather_em = results_weather_em.append({
    'Model': 'Gradient Boosting Classifier EM',
    'Accuracy': accuracy_gb,
    'Precision': precision_gb,
    'Recall': recall_gb,
    'F1-score': f1_gb,
    'ROC AUC Score': auc_gb,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_em
```

##### GRADIENT BOOSTING WITH SCALED DATA (EM)
```{python}
#| label: em imputation technique - gradient boosting scaled
#| eval: false

start_time = time.time()
# Initialize Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier()

# Fit the model
gb_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_gbs = gb_classifier.predict(X_test_scaled_weather)

# Calculate accuracy for the Gradient Boosting Classifier
accuracy_gbs = accuracy_score(y_test, y_pred_gbs)
precision_gbs = precision_score(y_test, y_pred_gbs)
recall_gbs = recall_score(y_test, y_pred_gbs)
f1_gbs = f1_score(y_test, y_pred_gbs)
auc_gbs = roc_auc_score(y_test, y_pred_gbs)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_em = results_weather_em.append({
    'Model': 'Gradient Boosting Classifier Scaled EM',
    'Accuracy': accuracy_gbs,
    'Precision': precision_gbs,
    'Recall': recall_gbs,
    'F1-score': f1_gbs,
    'ROC AUC Score': auc_gbs,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_em
```


##### KNEIGHBORS WITH ALL VARIABLES (EM)
```{python}
#| label: em imputation technique - knn
#| eval: false

X_test = np.array(X_test)

start_time = time.time()

# Initialize K-Nearest Neighbors Classifier
knn_classifier = KNeighborsClassifier()

# Fit the model
knn_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_knn = knn_classifier.predict(X_test)

# Calculate accuracy for the K-Nearest Neighbors Classifier
accuracy_knn = accuracy_score(y_test, y_pred_knn)
precision_knn = precision_score(y_test, y_pred_knn)
recall_knn = recall_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)
auc_knn = roc_auc_score(y_test, y_pred_knn)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_em = results_weather_em.append({
    'Model': 'KNN Classifier EM',
    'Accuracy': accuracy_knn,
    'Precision': precision_knn,
    'Recall': recall_knn,
    'F1-score': f1_knn,
    'ROC AUC Score': auc_knn,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_em
```

##### KNEIGHBORS WITH SCALED DATA (EM)
```{python}
#| label: em imputation technique - knn scaled
#| eval: false

X_test_scaled_weather = np.array(X_test_scaled_weather)

start_time = time.time()

# Fit the model
knn_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_knns = knn_classifier.predict(X_test_scaled_weather)

# Append the accuracy score for the second model to the DataFrame
accuracy_knns = accuracy_score(y_test, y_pred_knns)
precision_knns = precision_score(y_test, y_pred_knns)
recall_knns = recall_score(y_test, y_pred_knns)
f1_knns = f1_score(y_test, y_pred_knns)
auc_knns = roc_auc_score(y_test, y_pred_knns)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_em = results_weather_em.append({
    'Model': 'KNN Classifier Scaled EM',
    'Accuracy': accuracy_knns,
    'Precision': precision_knns,
    'Recall': recall_knns,
    'F1-score': f1_knns,
    'ROC AUC Score': auc_knns,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_em
```

##### ADABOOST WITH ALL VARIABLES (EM)
```{python}
#| label: em imputation technique - adaboost
#| eval: false

start_time = time.time()
# Initialize AdaBoost Classifier
adaboost_classifier = AdaBoostClassifier()

# Fit the model
adaboost_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_adaboost = adaboost_classifier.predict(X_test)

# Calculate accuracy for the AdaBoost Classifier
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
precision_adaboost = precision_score(y_test, y_pred_adaboost)
recall_adaboost = recall_score(y_test, y_pred_adaboost)
f1_adaboost = f1_score(y_test, y_pred_adaboost)
auc_adaboost = roc_auc_score(y_test, y_pred_adaboost)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_em = results_weather_em.append({
    'Model': 'AdaBoost Classifier EM',
    'Accuracy': accuracy_adaboost,
    'Precision': precision_adaboost,
    'Recall': recall_adaboost,
    'F1-score': f1_adaboost,
    'ROC AUC Score': auc_adaboost,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_em
```

##### ADABOOST WITH SCALED DATA (EM)
```{python}
#| label: em imputation technique - adaboost scaled
#| eval: false

start_time = time.time()

# Fit the model
adaboost_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_adaboost = adaboost_classifier.predict(X_test_scaled_weather)

# Calculate accuracy for the AdaBoost Classifier
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
precision_adaboost = precision_score(y_test, y_pred_adaboost)
recall_adaboost = recall_score(y_test, y_pred_adaboost)
f1_adaboost = f1_score(y_test, y_pred_adaboost)
auc_adaboost = roc_auc_score(y_test, y_pred_adaboost)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_em = results_weather_em.append({
    'Model': 'AdaBoost Classifier Scaled EM',
    'Accuracy': accuracy_adaboost,
    'Precision': precision_adaboost,
    'Recall': recall_adaboost,
    'F1-score': f1_adaboost,
    'ROC AUC Score': auc_adaboost,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_em
```

#### SHOWING THE MODEL PERFORMANCE
```{python}
#| label: em imputation technique - models performance
#| eval: false

results_weather_em
```

```{python}
#| label: em imputation technique - save model table
#| eval: false

# Save the DataFrame to a CSV file
results_weather_em.to_csv('photoDF/results_weather_em.csv', index=False)
```

Based on the provided metrics, **the Random Forest (Scaled)** appears to be performing the best for rain prediction in Australia when using Expectation Maximization (EM) imputation techniques. It achieves the highest values across most evaluation criteria, including accuracy, precision, F1-score, and ROC/AUC, despite it has a relatively high computational time compared to some other models.

### REGRESSION IMPUTATION TECHNIQUE

Regression imputation is a method of handling missing data by replacing them with estimated values derived from existing variables. This approach involves constructing a model using known values to calculate the relationship between variables (Hameed & Ali, 2023). This technique estimates missing values based on this regression, typically resulting in more accurate outcomes compared to mean imputation and deletion methods. Moreover, it helps minimize alterations to standard deviation and distribution shape. However, it does not introduce new information and may reduce standard error by increasing the sample size (Kang, 2013).

#### FILLING MISSING VALUES BY REGRESSION
```{python}
#| label: regression imputation technique - filling missing values
#| eval: false

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# Record starting time
start_time = time.time()

# Initialize IterativeImputer with regression strategy
imputer = IterativeImputer(random_state=0)

# Fit and transform the data
weather_filled_regression = pd.DataFrame(imputer.fit_transform(weather), columns=weather.columns)

# Record ending time
end_time = time.time()

# Calculate computational time
imputation_comp_time_this = end_time - start_time

# Append the result to the DataFrame
imputation_comp_time = imputation_comp_time.append(
            {'Imputation Technique': 'Regression',
            'Computational Time': imputation_comp_time_this},
            ignore_index=True)

# Checking for missing values
print("\nMissing values in each column:")
print(weather_filled_regression.isnull().sum())

# Visualizing missing data pattern
plt.figure(figsize=(12, 8))
sns.heatmap(weather_filled_regression.isnull(), cmap='viridis', cbar=False)
plt.title('Missing Data Pattern')
plt.show()
```

#### PREPROCESSING STEPS BEFORE MODELLING: SPLITTING THE DATA
```{python}
#| label: regression imputation technique - preprocessing
#| eval: false

X = weather_filled_regression.drop(columns = ['RainTomorrow'])
y = weather_filled_regression['RainTomorrow']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the training and testing data separately
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)

# Convert scaled data back to DataFrames
X_train_scaled_weather = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled_weather = pd.DataFrame(X_test_scaled, columns=X.columns)
```

#### MODELING WITH REGRESSION
##### LOGISTIC REGRESSION WITH ALL VARIABLES (REGRESSION)
```{python}
#| label: regression imputation technique - logistic regression
#| eval: false

start_time = time.time()

log_model = LogisticRegression()

log_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = log_model.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Creatin a performance scores dataframe
weather_filled_regression = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC Score', 'Computational Time'])

# Append the metrics to the DataFrame
weather_filled_regression = weather_filled_regression.append({
    'Model': 'Logistic Regression Classifier',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

weather_filled_regression
```

##### LOGISTIC REGRESSION WITH SCALED DATA (REGRESSION)
```{python}
#| label: regression imputation technique - logistic regression scaled
#| eval: false

start_time = time.time()

log_model.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = log_model.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
weather_filled_regression = weather_filled_regression.append({
    'Model': 'Logistic Regression Classifier Scaled',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

weather_filled_regression
```


##### DECISION TREE WITH ALL VARIABLES (REGRESSION)
```{python}
#| label: regression imputation technique - decision tree
#| eval: false

start_time = time.time()

# Create a Decision Tree Classifier
dt_clf = DecisionTreeClassifier()

# Fit the model to the training data
dt_clf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = dt_clf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
weather_filled_regression = weather_filled_regression.append({
    'Model': 'Decision Tree Classifier',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

weather_filled_regression
```

##### DECISION TREE WITH SCALED DATA (REGRESSION)
```{python}
#| label: regression imputation technique - decision tree scaled
#| eval: false

start_time = time.time()

# Fit the model to the training data
dt_clf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = dt_clf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time 

# Append the metrics to the DataFrame
weather_filled_regression = weather_filled_regression.append({
    'Model': 'Decision Tree Classifier Scaled',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

weather_filled_regression
```


##### RANDOM FOREST WITH ALL VARIABLES (REGRESSION)
```{python}
#| label: regression imputation technique - random forest
#| eval: false

start_time = time.time()

model_rf = RandomForestClassifier()
model_rf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
weather_filled_regression = weather_filled_regression.append({
    'Model': 'Random Forest Classifier',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

weather_filled_regression
```

##### RANDOM FOREST WITH SCALED DATA (REGRESSION)
```{python}
#| label: regression imputation technique - random forest scaled
#| eval: false

start_time = time.time()

# Fit the model to the training data
model_rf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
weather_filled_regression = weather_filled_regression.append({
    'Model': 'Random Forest Classifier Scaled',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

weather_filled_regression
```


##### GRADIENT BOOSTING WITH ALL VARIABLES (REGRESSION)
```{python}
#| label: regression imputation technique - gradient boosting
#| eval: false

start_time = time.time()

# Initialize Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier()

# Fit the model
gb_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_gb = gb_classifier.predict(X_test)

# Calculate accuracy for the Gradient Boosting Classifier
accuracy_gb = accuracy_score(y_test, y_pred_gb)
precision_gb = precision_score(y_test, y_pred_gb)
recall_gb = recall_score(y_test, y_pred_gb)
f1_gb = f1_score(y_test, y_pred_gb)
auc_gb = roc_auc_score(y_test, y_pred_gb)

end_time = time.time()
computational_time = end_time - start_time 

# Append the metrics to the DataFrame
weather_filled_regression = weather_filled_regression.append({
    'Model': 'Gradient Boosting Classifier',
    'Accuracy': accuracy_gb,
    'Precision': precision_gb,
    'Recall': recall_gb,
    'F1-score': f1_gb,
    'ROC AUC Score': auc_gb,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
weather_filled_regression
```

##### GRADIENT BOOSTING WITH SCALED DATA (REGRESSION)
```{python}
#| label: regression imputation technique - gradient boosting scaled
#| eval: false

start_time = time.time()
# Initialize Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier()

# Fit the model
gb_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_gbs = gb_classifier.predict(X_test_scaled_weather)

# Calculate accuracy for the Gradient Boosting Classifier
accuracy_gbs = accuracy_score(y_test, y_pred_gbs)
precision_gbs = precision_score(y_test, y_pred_gbs)
recall_gbs = recall_score(y_test, y_pred_gbs)
f1_gbs = f1_score(y_test, y_pred_gbs)
auc_gbs = roc_auc_score(y_test, y_pred_gbs)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
weather_filled_regression = weather_filled_regression.append({
    'Model': 'Gradient Boosting Classifier Scaled',
    'Accuracy': accuracy_gbs,
    'Precision': precision_gbs,
    'Recall': recall_gbs,
    'F1-score': f1_gbs,
    'ROC AUC Score': auc_gbs,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
weather_filled_regression
```


##### KNEIGHBORS WITH ALL VARIABLES (REGRESSION)
```{python}
#| label: regression imputation technique - knn
#| eval: false

X_test = np.array(X_test)

start_time = time.time()

# Initialize K-Nearest Neighbors Classifier
knn_classifier = KNeighborsClassifier()

# Fit the model
knn_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_knn = knn_classifier.predict(X_test)

# Calculate accuracy for the K-Nearest Neighbors Classifier
accuracy_knn = accuracy_score(y_test, y_pred_knn)
precision_knn = precision_score(y_test, y_pred_knn)
recall_knn = recall_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)
auc_knn = roc_auc_score(y_test, y_pred_knn)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
weather_filled_regression = weather_filled_regression.append({
    'Model': 'KNN Classifier',
    'Accuracy': accuracy_knn,
    'Precision': precision_knn,
    'Recall': recall_knn,
    'F1-score': f1_knn,
    'ROC AUC Score': auc_knn,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
weather_filled_regression
```

##### KNEIGHBORS WITH SCALED DATA (REGRESSION)
```{python}
#| label: regression imputation technique - knn scaled
#| eval: false

X_test_scaled_weather = np.array(X_test_scaled_weather)

start_time = time.time()

# Fit the model
knn_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_knns = knn_classifier.predict(X_test_scaled_weather)

# Append the accuracy score for the second model to the DataFrame
accuracy_knns = accuracy_score(y_test, y_pred_knns)
precision_knns = precision_score(y_test, y_pred_knns)
recall_knns = recall_score(y_test, y_pred_knns)
f1_knns = f1_score(y_test, y_pred_knns)
auc_knns = roc_auc_score(y_test, y_pred_knns)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
weather_filled_regression = weather_filled_regression.append({
    'Model': 'KNN Classifier Scaled',
    'Accuracy': accuracy_knns,
    'Precision': precision_knns,
    'Recall': recall_knns,
    'F1-score': f1_knns,
    'ROC AUC Score': auc_knns,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
weather_filled_regression
```


##### ADABOOST WITH ALL VARIABLES (REGRESSION)
```{python}
#| label: regression imputation technique - adaboost
#| eval: false

start_time = time.time()
# Initialize AdaBoost Classifier
adaboost_classifier = AdaBoostClassifier()

# Fit the model
adaboost_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_adaboost = adaboost_classifier.predict(X_test)

# Calculate accuracy for the AdaBoost Classifier
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
precision_adaboost = precision_score(y_test, y_pred_adaboost)
recall_adaboost = recall_score(y_test, y_pred_adaboost)
f1_adaboost = f1_score(y_test, y_pred_adaboost)
auc_adaboost = roc_auc_score(y_test, y_pred_adaboost)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
weather_filled_regression = weather_filled_regression.append({
    'Model': 'AdaBoost Classifier',
    'Accuracy': accuracy_adaboost,
    'Precision': precision_adaboost,
    'Recall': recall_adaboost,
    'F1-score': f1_adaboost,
    'ROC AUC Score': auc_adaboost,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
weather_filled_regression
```

##### ADABOOST WITH SCALED DATA (REGRESSION)
```{python}
#| label: regression imputation technique - adaboost scaled
#| eval: false

start_time = time.time()

# Fit the model
adaboost_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_adaboost = adaboost_classifier.predict(X_test_scaled_weather)

# Calculate accuracy for the AdaBoost Classifier
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
precision_adaboost = precision_score(y_test, y_pred_adaboost)
recall_adaboost = recall_score(y_test, y_pred_adaboost)
f1_adaboost = f1_score(y_test, y_pred_adaboost)
auc_adaboost = roc_auc_score(y_test, y_pred_adaboost)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
weather_filled_regression = weather_filled_regression.append({
    'Model': 'AdaBoost Classifier Scaled',
    'Accuracy': accuracy_adaboost,
    'Precision': precision_adaboost,
    'Recall': recall_adaboost,
    'F1-score': f1_adaboost,
    'ROC AUC Score': auc_adaboost,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
weather_filled_regression
```

#### SHOWING THE MODEL PERFORMANCE
```{python}
#| label: regression imputation technique - models performance
#| eval: false

weather_filled_regression
```

```{python}
#| label: regression imputation technique - save model table
#| eval: false

# Save the DataFrame to a CSV file
weather_filled_regression.to_csv('photoDF/weather_filled_regression.csv', index=False)
```

Based on the provided metrics, the Random Forest (Original) appears to be performing the best for rain prediction in Australia when using regression imputation techniques. It achieves the highest values across most evaluation criteria, including accuracy, precision, F1-score, and ROC/AUC, despite it has a relatively high computational time compared to some other models.

### INTERPOLATION IMPUTATION TECHNIQUE

There are several interpolation methods. First, linear interpolation involves estimating missing data by fitting a linear regression to the nearest data points before and after the gap. Second, moving mean interpolation replaces missing values with the mean of neighboring data points, calculated over a range centered around the missing value. Third, spline interpolation utilizes a spline regression to estimate missing data based on data points on both sides of the gap. Lastly, interpolation using temporal series analysis involves analyzing seasonality using data from previous years and estimating missing data through a linear regression between the current year's data and the seasonal trend. However, this paper use only linear interpolation and may not cover all of these techniques due to the limitation of the report's scope. (Picornell et al., 2021). 

#### FILLING MISSING VALUES BY LINEAR INTERPOLATION
```{python}
#| label: interpolation imputation technique - filling missing values
#| eval: false

# Record starting time
start_time = time.time()

# Apply linear interpolation for numerical columns
numeric_columns = weather.select_dtypes(include=['float64', 'int64']).columns
weather[numeric_columns] = weather[numeric_columns].interpolate(method='linear')

# Fit the imputer to your data and transform it
weather_filled_interpolation = imputer.fit_transform(weather)

# Convert the array back to a DataFrame if necessary
weather_filled_interpolation = pd.DataFrame(weather_filled_interpolation, columns=weather.columns)

# Record ending time
end_time = time.time()

# Calculate computational time
imputation_comp_time_this = end_time - start_time

# Append the result to the DataFrame
imputation_comp_time = imputation_comp_time.append(
            {'Imputation Technique': 'Interpolation',
            'Computational Time': imputation_comp_time_this},
            ignore_index=True)

# Checking for missing values
print("\nMissing values in each column:")
print(weather_filled_interpolation.isnull().sum())

# Visualizing missing data pattern
plt.figure(figsize=(12, 8))
sns.heatmap(weather_filled_interpolation.isnull(), cmap='viridis', cbar=False)
plt.title('Missing Data Pattern')
plt.show()
```

#### PREPROCESSING STEPS BEFORE MODELLING: SPLITTING THE DATA
```{python}
#| label: interpolation imputation technique - preprocessing
#| eval: false

X = weather_filled_interpolation.drop(columns = ['RainTomorrow'])
y = weather_filled_interpolation['RainTomorrow']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the training and testing data separately
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)

# Convert scaled data back to DataFrames
X_train_scaled_weather = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled_weather = pd.DataFrame(X_test_scaled, columns=X.columns)
```


#### MODELING WITH LINEAR INTERPOLATION
##### LOGISTIC REGRESSION WITH ALL VARIABLES (LINEAR INTERPOLATION)
```{python}
#| label: interpolation imputation technique - logistic regression
#| eval: false

# Creatin a performance scores dataframe
results_weather_interpolation = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC Score', 'Computational Time'])

# Start timing
start_time = time.time()

# Initialize Logistic Regression model
log_model = LogisticRegression()

# Fit the model on the training data
log_model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = log_model.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Logistic Regression with All Variables (Interpolation)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_interpolation = pd.concat([results_weather_interpolation, new_row], ignore_index=True)

results_weather_interpolation
```

##### LOGISTIC REGRESSION WITH SCALED DATA (LINEAR INTERPOLATION)
```{python}
#| label: interpolation imputation technique - logistic regression scaled
#| eval: false

# Start timing
start_time = time.time()

# Fit the model on the training data
log_model.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = log_model.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Logistic Regression with Scaled Data (Interpolation)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_interpolation = pd.concat([results_weather_interpolation, new_row], ignore_index=True)

results_weather_interpolation
```


##### DECISION TREE WITH ALL VARIABLES (LINEAR INTERPOLATION)
```{python}
#| label: interpolation imputation technique - decision tree
#| eval: false

# Start timing
start_time = time.time()

# Initialize Decision Tree model
dt_clf = DecisionTreeClassifier()

# Fit the model on the training data
dt_clf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = dt_clf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Decision Tree with All Variables (Interpolation)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_interpolation = pd.concat([results_weather_interpolation, new_row], ignore_index=True)

results_weather_interpolation
```

##### DECISION TREE WITH SCALED DATA (LINEAR INTERPOLATION)
```{python}
#| label: interpolation imputation technique - decision tree scaled
#| eval: false

# Start timing
start_time = time.time()

# Fit the model on the training data
dt_clf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = dt_clf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Decision Tree with Scaled Data (Interpolation)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_interpolation = pd.concat([results_weather_interpolation, new_row], ignore_index=True)

results_weather_interpolation
```


##### RANDOM FOREST WITH ALL VARIABLES (LINEAR INTERPOLATION)
```{python}
#| label: interpolation imputation technique - random forest
#| eval: false

# Start timing
start_time = time.time()

# Initialize Random Forest model
model_rf = RandomForestClassifier()

# Fit the model on the training data
model_rf.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = model_rf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Random Forest with All Variables (Interpolation)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_interpolation = pd.concat([results_weather_interpolation, new_row], ignore_index=True)

results_weather_interpolation
```

##### RANDOM FOREST WITH SCALED DATA (LINEAR INTERPOLATION)
```{python}
#| label: interpolation imputation technique - random forest scaled
#| eval: false

# Start timing
start_time = time.time()

# Fit the model on the training data
model_rf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Random Forest with Scaled Data (Interpolation)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_interpolation = pd.concat([results_weather_interpolation, new_row], ignore_index=True)

results_weather_interpolation
```


##### GRADIENT BOOSTING WITH ALL VARIABLES (LINEAR INTERPOLATION)
```{python}
#| label: interpolation imputation technique - gradient boosting
#| eval: false

# Start timing
start_time = time.time()

# Initialize Gradient Boosting model
gb_classifier = GradientBoostingClassifier()

# Fit the model on the training data
gb_classifier.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = gb_classifier.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Gradient Boosting with All Variables (Interpolation)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_interpolation = pd.concat([results_weather_interpolation, new_row], ignore_index=True)

results_weather_interpolation
```

##### GRADIENT BOOSTING WITH SCALED DATA (LINEAR INTERPOLATION)
```{python}
#| label: interpolation imputation technique - gradient boosting scaled
#| eval: false

# Start timing
start_time = time.time()

# Fit the model on the training data
gb_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = gb_classifier.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'Gradient Boosting with Scaled Data (Interpolation)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_interpolation = pd.concat([results_weather_interpolation, new_row], ignore_index=True)

results_weather_interpolation
```


##### KNEIGHBORS WITH ALL VARIABLES (LINEAR INTERPOLATION)
```{python}
#| label: interpolation imputation technique - knn
#| eval: false

X_test = np.array(X_test)
# Start timing
start_time = time.time()

# Initialize Logistic Regression model
knn_classifier = KNeighborsClassifier()

# Fit the model on the training data
knn_classifier.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = knn_classifier.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'KNeighbors with All Variables (Interpolation)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_interpolation = pd.concat([results_weather_interpolation, new_row], ignore_index=True)

results_weather_interpolation
```

##### KNEIGHBORS WITH SCALED DATA (LINEAR INTERPOLATION)
```{python}
#| label: interpolation imputation technique - knn scaled
#| eval: false

X_train_scaled_weather = np.array(X_train_scaled_weather)
# Start timing
start_time = time.time()

# Fit the model on the training data
knn_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = knn_classifier.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'KNeighbors with Scaled Data (Interpolation)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_interpolation = pd.concat([results_weather_interpolation, new_row], ignore_index=True)

results_weather_interpolation
```


##### ADABOOST WITH ALL VARIABLES (LINEAR INTERPOLATION)
```{python}
#| label: interpolation imputation technique - adaboost
#| eval: false

# Start timing
start_time = time.time()

# Initialize Logistic Regression model
adaboost_classifier = AdaBoostClassifier()

# Fit the model on the training data
adaboost_classifier.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = adaboost_classifier.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'AdaBoost with All Variables (Interpolation)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_interpolation = pd.concat([results_weather_interpolation, new_row], ignore_index=True)

results_weather_interpolation
```

##### ADABOOST WITH SCALED DATA (LINEAR INTERPOLATION)
```{python}
#| label: interpolation imputation technique - adaboost scaled
#| eval: false

# Start timing
start_time = time.time()

# Fit the model on the training data
adaboost_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = adaboost_classifier.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

# Calculate computational time
end_time = time.time()
computational_time = end_time - start_time

# Create a new row as a DataFrame to append
new_row = pd.DataFrame([{
    'Model': 'AdaBoost with Scaled Data (Interpolation)',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}])

# Append the new row using pd.concat
results_weather_interpolation = pd.concat([results_weather_interpolation, new_row], ignore_index=True)

results_weather_interpolation
```


#### SHOWING THE MODEL PERFORMANCE WITH LINEAR INTERPOLATION
```{python}
#| label: interpolation imputation technique - models performance
#| eval: false

results_weather_interpolation
```

Now, apart from the imputation techniques, I want to see that how the models will performs if i delete all the observations (rows) that contain a missing value in certain variables.

```{python}
#| label: interpolation imputation technique - save model table
#| eval: false

# Save the DataFrame to a CSV file
results_weather_interpolation.to_csv('photoDF/results_weather_interpolation.csv', index=False)
```

Based on the provided metrics, the Random Forest (Scaled) appears to be performing the best for rain prediction in Australia when using linear interpolation imputation techniques. It achieves the highest values across most evaluation criteria, including accuracy, precision, F1-score, and ROC/AUC. However, we can notice an anomaly in the computational time for this model as well, which is significantly higher compared to other models. This anomaly should be investigated further to ensure data integrity and consistency.

### IGNORING MISSING VALUE STRATEGY - LISTWISE DELETION 

This approach has two common methods. First, listwise deletion, also referred to as case deletion, casewise deletion, and complete case analysis, involves excluding entire observations with missing values for any variable (Song & Shepperd, 2007). This method is commonly used and simple but leads to loss of data and potential bias if the data are not missing completely at random (MCAR). On the other hand, pairwise deletion keeps as many data points as it can for each analysis. It focuses only on the important variables for each case, without considering the rest of the dataset. While it preserves more data than listwise deletion, it still results in some loss of information. This approach is less biased for MCAR or missing at random (MAR) data but may be inadequate for analyses with many missing observations (Kang, 2013). In this project, we will use listwise deletion as a strategy for handling missing values in this dataset. 

```{python}
#| label: listwise deletion imputation technique
#| eval: false

# Record starting time
start_time = time.time()

# Drop rows with any missing values (listwise deletion)
weather_ld = weather.dropna()

# Record ending time
end_time = time.time()

# Calculate computational time
imputation_comp_time_this = end_time - start_time

# Append the result to the DataFrame
imputation_comp_time = imputation_comp_time.append(
            {'Imputation Technique': 'Listwise deletion',
            'Computational Time': imputation_comp_time_this},
            ignore_index=True)

# Checking for missing values
print("\nMissing values in each column:")
print(weather_ld.isnull().sum())

# Visualizing missing data pattern
plt.figure(figsize=(12, 8))
sns.heatmap(weather_ld.isnull(), cmap='viridis', cbar=False)
plt.title('Missing Data Pattern - After Listwise Deletion')
plt.show()
```

```{python}
#| label: listwise deletion imputation technique - preprocessing
#| eval: false

X = weather_ld.drop(columns = ['RainTomorrow'])
y = weather_ld['RainTomorrow']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the training and testing data separately
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)

# Convert scaled data back to DataFrames
X_train_scaled_weather = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled_weather = pd.DataFrame(X_test_scaled, columns=X.columns)

```

#### MODELING WITH LD
##### LOGISTIC REGRESSION (LD)

```{python}
#| label: listwise deletion imputation technique - logistic regression
#| eval: false

start_time = time.time()

log_model = LogisticRegression()

log_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = log_model.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Creatin a performance scores dataframe
results_weather_ld = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC Score', 'Computational Time'])

# Append the metrics to the DataFrame
results_weather_ld = results_weather_ld.append({
    'Model': 'Logistic Regression Classifier EM',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_ld
```

##### LOGISTIC REGRESSION WITH SCALED DATA (LD)

```{python}
#| label: listwise deletion imputation technique - logistic regression scaled
#| eval: false

start_time = time.time()

log_model.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = log_model.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_ld = results_weather_ld.append({
    'Model': 'Logistic Regression Classifier Scaled EM',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_ld
```


##### DECISION TREE WITH ALL VARIABLES (LD)

```{python}
#| label: listwise deletion imputation technique - decision tree
#| eval: false

start_time = time.time()

# Create a Decision Tree Classifier
dt_clf = DecisionTreeClassifier()

# Fit the model to the training data
dt_clf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = dt_clf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_ld = results_weather_ld.append({
    'Model': 'Decision Tree Classifier EM',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_ld
```

##### DECISION TREE WITH SCALED DATA (LD)
```{python}
#| label: listwise deletion imputation technique - decision tree scaled
#| eval: false

start_time = time.time()

# Fit the model to the training data
dt_clf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = dt_clf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time 

# Append the metrics to the DataFrame
results_weather_ld = results_weather_ld.append({
    'Model': 'Decision Tree Classifier Scaled EM',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_ld
```


##### RANDOM FOREST WITH ALL VARIABLES (LD)
```{python}
#| label: listwise deletion imputation technique - random forest
#| eval: false

start_time = time.time()

model_rf = RandomForestClassifier()
model_rf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_ld = results_weather_ld.append({
    'Model': 'Random Forest Classifier EM',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_ld
```

##### RANDOM FOREST WITH SCALED DATA (LD)
```{python}
#| label: listwise deletion imputation technique - random forest scaled
#| eval: false

start_time = time.time()

# Fit the model to the training data
model_rf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_ld = results_weather_ld.append({
    'Model': 'Random Forest Classifier Scaled Mean',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_ld
```


##### GRADIENT BOOSTING WITH ALL VARIABLES (LD)
```{python}
#| label: listwise deletion imputation technique - gradient boosting
#| eval: false

start_time = time.time()

# Initialize Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier()

# Fit the model
gb_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_gb = gb_classifier.predict(X_test)

# Calculate accuracy for the Gradient Boosting Classifier
accuracy_gb = accuracy_score(y_test, y_pred_gb)
precision_gb = precision_score(y_test, y_pred_gb)
recall_gb = recall_score(y_test, y_pred_gb)
f1_gb = f1_score(y_test, y_pred_gb)
auc_gb = roc_auc_score(y_test, y_pred_gb)

end_time = time.time()
computational_time = end_time - start_time 

# Append the metrics to the DataFrame
results_weather_ld = results_weather_ld.append({
    'Model': 'Gradient Boosting Classifier EM',
    'Accuracy': accuracy_gb,
    'Precision': precision_gb,
    'Recall': recall_gb,
    'F1-score': f1_gb,
    'ROC AUC Score': auc_gb,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_ld
```

##### GRADIENT BOOSTING WITH SCALED DATA (LD)
```{python}
#| label: listwise deletion imputation technique - gradient boosting scaled
#| eval: false

start_time = time.time()
# Initialize Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier()

# Fit the model
gb_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_gbs = gb_classifier.predict(X_test_scaled_weather)

# Calculate accuracy for the Gradient Boosting Classifier
accuracy_gbs = accuracy_score(y_test, y_pred_gbs)
precision_gbs = precision_score(y_test, y_pred_gbs)
recall_gbs = recall_score(y_test, y_pred_gbs)
f1_gbs = f1_score(y_test, y_pred_gbs)
auc_gbs = roc_auc_score(y_test, y_pred_gbs)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_ld = results_weather_ld.append({
    'Model': 'Gradient Boosting Classifier Scaled EM',
    'Accuracy': accuracy_gbs,
    'Precision': precision_gbs,
    'Recall': recall_gbs,
    'F1-score': f1_gbs,
    'ROC AUC Score': auc_gbs,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_ld
```


##### KNEIGHBORS WITH ALL VARIABLES (LD)
```{python}
#| label: listwise deletion imputation technique - knn
#| eval: false

X_test = np.array(X_test)

start_time = time.time()

# Initialize K-Nearest Neighbors Classifier
knn_classifier = KNeighborsClassifier()

# Fit the model
knn_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_knn = knn_classifier.predict(X_test)

# Calculate accuracy for the K-Nearest Neighbors Classifier
accuracy_knn = accuracy_score(y_test, y_pred_knn)
precision_knn = precision_score(y_test, y_pred_knn)
recall_knn = recall_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)
auc_knn = roc_auc_score(y_test, y_pred_knn)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_ld = results_weather_ld.append({
    'Model': 'KNN Classifier EM',
    'Accuracy': accuracy_knn,
    'Precision': precision_knn,
    'Recall': recall_knn,
    'F1-score': f1_knn,
    'ROC AUC Score': auc_knn,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_ld
```

##### KNEIGHBORS WITH SCALED DATA (LD)
```{python}
#| label: listwise deletion imputation technique - knn scaled
#| eval: false

X_test_scaled_weather = np.array(X_test_scaled_weather)

start_time = time.time()

# Fit the model
knn_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_knns = knn_classifier.predict(X_test_scaled_weather)

# Append the accuracy score for the second model to the DataFrame
accuracy_knns = accuracy_score(y_test, y_pred_knns)
precision_knns = precision_score(y_test, y_pred_knns)
recall_knns = recall_score(y_test, y_pred_knns)
f1_knns = f1_score(y_test, y_pred_knns)
auc_knns = roc_auc_score(y_test, y_pred_knns)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_ld = results_weather_ld.append({
    'Model': 'KNN Classifier Scaled EM',
    'Accuracy': accuracy_knns,
    'Precision': precision_knns,
    'Recall': recall_knns,
    'F1-score': f1_knns,
    'ROC AUC Score': auc_knns,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_ld
```

##### ADABOOST WITH ALL VARIABLES (LD)
```{python}
#| label: listwise deletion imputation technique - adaboost
#| eval: false

start_time = time.time()
# Initialize AdaBoost Classifier
adaboost_classifier = AdaBoostClassifier()

# Fit the model
adaboost_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_adaboost = adaboost_classifier.predict(X_test)

# Calculate accuracy for the AdaBoost Classifier
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
precision_adaboost = precision_score(y_test, y_pred_adaboost)
recall_adaboost = recall_score(y_test, y_pred_adaboost)
f1_adaboost = f1_score(y_test, y_pred_adaboost)
auc_adaboost = roc_auc_score(y_test, y_pred_adaboost)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_ld = results_weather_ld.append({
    'Model': 'AdaBoost Classifier EM',
    'Accuracy': accuracy_adaboost,
    'Precision': precision_adaboost,
    'Recall': recall_adaboost,
    'F1-score': f1_adaboost,
    'ROC AUC Score': auc_adaboost,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_ld
```

##### ADABOOST WITH SCALED DATA (LD)
```{python}
#| label: listwise deletion imputation technique - adaboost scaled
#| eval: false

start_time = time.time()

# Fit the model
adaboost_classifier.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred_adaboost = adaboost_classifier.predict(X_test_scaled_weather)

# Calculate accuracy for the AdaBoost Classifier
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
precision_adaboost = precision_score(y_test, y_pred_adaboost)
recall_adaboost = recall_score(y_test, y_pred_adaboost)
f1_adaboost = f1_score(y_test, y_pred_adaboost)
auc_adaboost = roc_auc_score(y_test, y_pred_adaboost)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_ld = results_weather_ld.append({
    'Model': 'AdaBoost Classifier Scaled EM',
    'Accuracy': accuracy_adaboost,
    'Precision': precision_adaboost,
    'Recall': recall_adaboost,
    'F1-score': f1_adaboost,
    'ROC AUC Score': auc_adaboost,
    'Computational Time': computational_time
}, ignore_index=True)

# Print the updated DataFrame
results_weather_ld
```

#### SHOWING THE MODEL PERFORMANCE

```{python}
#| label: listwise deletion imputation technique - models performance
#| eval: false

results_weather_ld
```

```{python}
#| label: listwise deletion imputation technique - save model table
#| eval: false

# Save the DataFrame to a CSV file
results_weather_ld.to_csv('photoDF/results_weather_ld.csv', index=False)
```

Based on the provided metrics, **the Random Forest (Scaled) model** appears to be performing the best for rain prediction in Australia when using listwise deletion, as it achieves the highest accuracy, precision, F1-score, and ROC/AUC. Additionally, the computational time for the Random Forest (Scaled) model is reasonable compared to other models, making it a strong choice for this strategy.

### BEST MODEL SUMMARY OF EACH IMPUTATION AND STRATEGY

Regarding the overall model performance as shown previously, we would like to summarize the best model performance of each imputation and strategy below. 

```{python}
#| label: saving the imputation computational time dataframe
#| eval: false

# Save the DataFrame to a CSV file
imputation_comp_time.to_csv('photoDF/imputation_comp_time.csv', index=False)
```

As the table shown above, the combination of MICE imputation technique with **the Random Forest (Scaled) model** yields the highest accuracy, precision, recall, F1-score, and ROC/AUC among the presented options. Additionally, it achieves these metrics with a reasonable computational time, making it the best choice for rain prediction in Australia among the listed techniques and models.

# ANALYSIS AND RESULTS

Having identified the MICE Imputation Technique with Random Forest Scaling as the optimal performer within this dataset, we want to focus on delving deeper into our analysis, using this technique and model.

## Rerun the Mice Imputation Technique
```{python}
#| label: continuity - mice imputation technique - filling missing values

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# Initialize the IterativeImputer with appropriate parameters
imputer = IterativeImputer(max_iter=10, random_state=0)

# Fit the imputer to the data and transform the dataset
weather = pd.DataFrame(imputer.fit_transform(weather), columns=weather.columns)

# Checking for missing values
print("\nMissing values in each column after MICE imputation:")
print(weather.isnull().sum())

# Visualizing missing data pattern
plt.figure(figsize=(12, 8))
sns.heatmap(weather.isnull(), cmap='viridis', cbar=False)
plt.title('Missing Data Pattern after MICE Imputation')
plt.show()
```

## Exploratory Data Analysis

### Correlation Heatmap for the numerical features

This code generates a correlation heatmap for the weather dataset, providing a visual representation of the relationships between different features. A correlation heatmap is a graphical representation of the correlation matrix, where each cell's color indicates the strength and direction of the correlation between two variables. Moreover, the correlation matrix shows the linear relationship between pairs of features in the dataset. The colormap 'YlOrBr' is applied to represent the strength of correlations, with darker shades indicating stronger correlations. Additionally, the code saves the generated heatmap as a PNG file for further analysis and visualization purposes.

```{python}
#| label: continuity - Correlation heatmap for the weather dataset 
#| echo: false
#| eval: false

# Calculate correlation matrix
correlation_matrix = weather.corr().round(2)

# Set up the matplotlib figure
plt.figure(figsize=(15, 10))

# Draw the heatmap with seaborn
sns.heatmap(correlation_matrix, annot=True, cmap='YlOrBr', fmt=".2f")

# Set the title and labels
plt.title('Correlation Heatmap for the Weather Dataset')
plt.xlabel('Features')
plt.ylabel('Features')

# Save the figure as a PNG file
plt.savefig('photoDF/correlation_heatmap.png')

# Display the heatmap
plt.show()
```


We can see that variables that show the status in different time of the day are strongly correlated with each other, which means that if the temperature is high at 9AM is it expected to be high also during 3PM, and vice versa.

```{python}
#| label: continuity - weather data infoo
#| echo: false
#| message: false
#| include: false

weather.info()
```


```{python}
#| label: continuity - weather headd
#| echo: false
#| message: false
#| include: false

weather.head(5)
```

### Boxplot of the numerical features 

We now turn our attention to visualizing the distribution of numerical features within the dataset. This step is crucial as it allows us to identify any outliers that may impact our overall predictive task. By visualizing boxplots, we want to gain insights into the variability and distribution of these features, paving the way for a more informed analysis.

```{python}
#| label: Visualizing the boxplots for the numerical variables of the weather's dataset 
#| echo: false
#| eval: false

numerical_columns2 = weather.select_dtypes(include=['int64', 'float64']).columns
num_plots_per_row = 3
num_rows = -(-len(numerical_columns2) // num_plots_per_row)  

plt.figure(figsize=(20, 4 * num_rows))

for i, column in enumerate(numerical_columns2, start=1):
    plt.subplot(num_rows, num_plots_per_row, i)
    sns.boxplot(x=weather[column], palette='Set3')
    plt.title(f"Boxplot for {column}")

plt.tight_layout()
# Save the plot as an image file
plt.savefig('photoDF/boxplotnumericalfeatures.png')
plt.show()
```

We can see that for the rainfall which is measured in millimetres there are a lot of outliers but the furthest one is 300mm which could indicate a real day when there was raining that amount of rain in a specific area, and then for the wind speed we can see that there are outliers until 80 km/h which could be logical. When looking at each of the variables the outliers make sense, and therefore we do not want them to remove.

### Histogram for the numerical features 

Following our examination of boxplots, we now go into further analysis by visualizing the distribution of numerical features through histograms. This step is essential for assessing the skewness of the variables, providing insights into their distribution characteristics.

```{python}
#| label: weather data histogram
#| echo: false
#| eval: false


plt.figure(figsize=(20, 4 * num_rows))

for i, column in enumerate(numerical_columns2, start=1):
    plt.subplot(num_rows, num_plots_per_row, i)
    sns.histplot(x=weather[column], palette='Set3')
    plt.title(f"Boxplot for {column}")

plt.tight_layout()
plt.savefig('photoDF/histogramnumericalfeatures.png')
plt.show()
```

We can see that most of the numerical features show a normal distribution, only for the three plots in the second row, we can see that they are sightly left-skewed.

## Weather: Modelling

### Modeling Summary

We will now revisit the Random Forest Scaled model and additionally employ its feature selection variant to conduct a comparative analysis. 
SelectKBest acts like a filter that sifts through a heap of data to identify the most crucial pieces of information. Picture having various features such as temperature, humidity, and wind speed, and wanting to pinpoint which ones are most valuable for predicting if it will rain tomorrow. SelectKBest helps in this regard.
It evaluates each piece of information, assigning a score based on its relevance to predicting future rainfall. For instance, if high humidity frequently precedes rain, humidity would likely receive a high score. Once all features are scored, SelectKBest cherry-picks the top few (K) with the highest scores.
Next, we use these top-ranking features to train a specialized computer program called a Random Forest. This program learns from the data to forecast whether it will rain tomorrow, based on factors like temperature, humidity, and wind speed.
Following training with the selected features, we assess the Random Forest's performance in predicting rainfall. 

```{python}
#| label: weather data splitting
#| echo: false
#| message: false
#| include: false
#| eval: false 

X = weather.drop(columns = ['RainTomorrow'])
y = weather['RainTomorrow']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the training data
scaler.fit(X_train)

# Transform the training and testing data separately
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert scaled data back to DataFrames
X_train_scaled_weather = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled_weather = pd.DataFrame(X_test_scaled, columns=X.columns)
```

```{python}
#| label: weather data random forest all variables scaled
#| echo: false
#| message: false
#| include: false
#| eval: false 

# Creatin a performance scores dataframe
results_weather_featureselection = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC Score', 'Computational Time'])

start_time = time.time()

# Fit the model to the training data
model_rf = RandomForestClassifier()
model_rf.fit(X_train_scaled_weather, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test_scaled_weather)

# Calculate performance metrics
accuracy_rfs = accuracy_score(y_test, y_pred)
precision_rfs = precision_score(y_test, y_pred)
recall_rfs = recall_score(y_test, y_pred)
f1_rfs = f1_score(y_test, y_pred)
auc_rfs = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_featureselection = results_weather_featureselection.append({
    'Model': 'Random Forest Classifier Scaled',
    'Accuracy': accuracy_rfs,
    'Precision': precision_rfs,
    'Recall': recall_rfs,
    'F1-score': f1_rfs,
    'ROC AUC Score': auc_rfs,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_featureselection
```

Therefore, we apply the SelectKBest algorithm to check which are the ten best features of the 19 that the dataset has.
```{python}
#| label: weather data selectkbest
#| echo: false
#| message: false
#| include: false
#| eval: false 
 
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression
target_variable = 'RainTomorrow'
k=10
X_feature_weather = weather.drop(columns=[target_variable])
y_feature_weather = weather[target_variable]
selector = SelectKBest(score_func=f_regression, k=k)
X_selected = selector.fit_transform(X_feature_weather, y_feature_weather)
selected_feature_indices = selector.get_support(indices=True)
selected_feature_names = weather.columns[selected_feature_indices].tolist()
print("Selected Features using SelectKBest:")
print(selected_feature_names)


X_train_featureselection_scaled_weather = X_train_scaled_weather.drop(columns = ['Location', 'MinTemp', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'Temp9am'])
X_test_featureselection_scaled_weather = X_test_scaled_weather.drop(columns = ['Location', 'MinTemp', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'Temp9am'])
```

The best variables are ***'MaxTemp'***, ***'Rainfall'***, ***'WindGustSpeed'***, ***'WindSpeed3pm'***, ***'Humidity9am'***, ***'Humidity3pm'***, ***'Pressure9am'***, ***'Pressure3pm'***, ***'Temp3pm'***, ***'RainToday'***.

Now we are going to perform the Random Forest model with the best variables:
```{python}
#| label: weather data random forest feature selection scaled
#| echo: false
#| message: false
#| include: false
#| eval: false 

start_time = time.time()

# Fit the model to the training data
model_rf.fit(X_train_featureselection_scaled_weather, y_train)

# Make predictions on the test data
y_pred = model_rf.predict(X_test_featureselection_scaled_weather)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

end_time = time.time()
computational_time = end_time - start_time

# Append the metrics to the DataFrame
results_weather_featureselection = results_weather_featureselection.append({
    'Model': 'Random Forest Classifier with Feature Selection Scaled',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc,
    'Computational Time': computational_time
}, ignore_index=True)

results_weather_featureselection
```

```{python}
#| label: weather data results table save it
#| echo: false
#| message: true
#| include: false
#| eval: false 

# Save the DataFrame to a CSV file
results_weather_featureselection.to_csv('photoDF/results_weather_featureselection.csv', index=False)
```

According to the results, the model didn't improve its performance by using the best variables. The reason why our model didn’t improve when using SelectKBest depends on these various factors:
    *Relevance of Features*: It may have happened due to the importance of the other features that were not used in the model. Those features could have had important information that the model needed to pick, therefore, removing them didn’t help the model.
    *Noise in Data*: Sometimes, features may contain noise or irrelevant information that can degrade the performance of the model. SelectKBest can help by filtering out these noisy features, leading to an improvement in model performance. Therefore, in our case, this means that our features that we didn’t select, didn't contain noise or irrelevant information.
    *Interaction among Features*: SelectKBest does not consider interactions among features. In our case, our features are quite correlated with each other, especially the ones that are the same but at a different hour. Consequently, selecting individual features may not capture the full predictive power of the data.
Therefore, we are going to only use the Random Forest Scaled with all the variables for further analysis, since it performs better.


### Best Model Performance 

**Cross-validation** is a technique used to evaluate the performance of a machine learning model by splitting the dataset into multiple subsets, or folds. In each iteration of cross-validation, one fold is used as the validation set, while the remaining folds are used as the training set. This process is repeated multiple times, with each fold serving as the validation set exactly once. Cross-validation helps to assess how well the model generalizes to unseen data by providing a more robust estimate of its performance compared to a single train-test split.

For this reason, we're now interested in seeing how Random Forest with scaled data performs, both with and without cross-validation. Will the results remain consistent, or will there be differences?

```{python}
#| label: weather data random forest all variables scaled
#| echo: false
#| message: false
#| include: false
#| eval: false

# Creatin a performance scores dataframe
results_weather_crossvalidation = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC AUC Score'])

# Append the metrics to the DataFrame
results_weather_crossvalidation = results_weather_crossvalidation.append({
    'Model': 'Random Forest Classifier Scaled',
    'Accuracy': accuracy_rfs,
    'Precision': precision_rfs,
    'Recall': recall_rfs,
    'F1-score': f1_rfs,
    'ROC AUC Score': auc_rfs
}, ignore_index=True)

results_weather_crossvalidation
```

```{python}
#| label: weather data random forest scaled cv
#| echo: false
#| message: false
#| include: false
#| eval: false

# Perform cross-validation
cv_results = cross_validate(model_rf, X_train_scaled_weather, y_train, cv=5, 
                            scoring=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'])

# Calculate mean scores across folds
accuracy = cv_results['test_accuracy'].mean()
precision = cv_results['test_precision'].mean()
recall = cv_results['test_recall'].mean()
f1 = cv_results['test_f1'].mean()
auc = cv_results['test_roc_auc'].mean()

# Append the metrics to the DataFrame
results_weather_crossvalidation = results_weather_crossvalidation.append({
    'Model': 'Random Forest Classifier Scaled with Cross Validation',
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-score': f1,
    'ROC AUC Score': auc
}, ignore_index=True)

results_weather_crossvalidation
```

```{python}
#| label: weather data results table save it
#| echo: false
#| message: true
#| include: false
#| eval: false 

# Save the DataFrame to a CSV file
results_weather_crossvalidation.to_csv('photoDF/results_weather_crossvalidation.csv', index=False)
```

Overall, from the table above, we can see that both models have similar accuracy and performance in terms of precision, recall, and F1-score, the second model with cross-validation demonstrates superior discrimination between classes, as evidenced by its higher ROC AUC score.

When the results of a model with and without cross-validation are almost the same, it means the model is consistent and doesn't rely heavily on how the data is split for validation. This suggests the model is stable and can generalize well to new data.

## Weather: Additional Techniques

### Learning Curve

A learning curve is applied to illustrate how well a model performs based on the amount of training data. It helps identify learning issues like underfitting or overfitting and assesses dataset representativeness. By comparing training and validation scores across different training set sizes, learning curves reveal how much the model improves with more data and whether its limitations are due to bias or variance errors (Giola et al., 2021).
Now, let's delve into exploring the learning curve of our best-performing model.

```{python}
#| label: weather data learning curve
#| echo: false
#| message: false
#| include: false
#| eval: false

## LEARNING CURVE FOR RANDOM FOREST CLASSIFIER

from sklearn.datasets import load_digits
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import LearningCurveDisplay, ShuffleSplit

fig, ax = plt.subplots(1, 1, figsize=(10, 6), sharey=True)

common_params = {
    "X": X,
    "y": y,
    "train_sizes": np.linspace(0.1, 1.0, 5),
    "cv": ShuffleSplit(n_splits=50, test_size=0.2),
    "score_type": "both",
    "n_jobs": 4,
    "line_kw": {"marker": "o"},
    "std_display_style": "fill_between",
    "score_name": "Accuracy",
}

estimator = model_rf  # Use only model_rf

LearningCurveDisplay.from_estimator(estimator, **common_params, ax=ax)
handles, label = ax.get_legend_handles_labels()
ax.legend(handles[:2], ["Training Score", "Test Score"])
ax.set_title(f"Learning Curve for {estimator.__class__.__name__}")

# Save the plot as an image file
plt.savefig('photoDF/learning_curve_rf_weather.png')

# Show the plot
plt.show()

```


The learning curve plot, as shown here, reveals that the model achieves a perfect score when trained on 20,000 samples, indicating its ability to memorize the training data entirely. However, the most significant improvement in performance for the testing data occurs when using over 100,000 samples. Beyond this point, additional samples result in minimal enhancements in performance.

### Checking for Overfitting 

Overfitting occurs when a model accurately captures the intricacies of the training data but struggles to generalize to new data from the same population, often because it learns patterns that are specific to the training data but not representative of the overall population. It can also refer to a model that is excessively complex for the given data and problem. Some define overfitting as the model learning noise present in the training data but not in the population. (Aliferis & Simon, 2024). 
Now we want to check overfitting with the Random Forest Classifier Scaled Dataset using different settings: max depth ranging from 1 to 20, and the number of estimators set at 50, 100, and 150. By testing various configurations, we aim to understand how the model's performance changes with different complexities. This helps us identify the optimal balance between model complexity and generalization ability.

```{python}
#| label: weather data max depth overfitting50
#| echo: false
#| message: true
#| include: false
#| eval: false

X = weather.drop(columns=['RainTomorrow'])
y = weather['RainTomorrow']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the training data
scaler.fit(X_train)

# Transform the training and testing data separately
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert scaled data back to DataFrames
X_train_scaled_weather = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled_weather = pd.DataFrame(X_test_scaled, columns=X.columns)

# Define lists to collect scores
train_scores, test_scores = [], []
train_precision, test_precision = [], []
train_recall, test_recall = [], []
train_f1, test_f1 = [], []
train_roc_auc_scores, test_roc_auc_scores = [], [] # Changed variable names

# Define the tree depths to evaluate
values = [i for i in range(1, 21)]

# Evaluate a random forest for each depth
for i in values:
    # Configure the model
    model = RandomForestClassifier(n_estimators=50, max_depth=i, random_state=42)
    
    # Fit model on the training dataset
    model.fit(X_train_scaled_weather, y_train)
    
    # Predictions
    train_yhat = model.predict(X_train_scaled_weather)
    test_yhat = model.predict(X_test_scaled_weather)
    
    # Accuracy
    train_acc = accuracy_score(y_train, train_yhat)
    test_acc = accuracy_score(y_test, test_yhat)
    
    # Precision
    train_prec = precision_score(y_train, train_yhat)
    test_prec = precision_score(y_test, test_yhat)
    
    # Recall
    train_rec = recall_score(y_train, train_yhat)
    test_rec = recall_score(y_test, test_yhat)
    
    # F1 Score
    train_f1score = f1_score(y_train, train_yhat)
    test_f1score = f1_score(y_test, test_yhat)
    
    # ROC AUC Score
    train_roc_auc = roc_auc_score(y_train, train_yhat)
    test_roc_auc = roc_auc_score(y_test, test_yhat)
    
    # Append scores to respective lists
    train_scores.append(train_acc)
    test_scores.append(test_acc)
    train_precision.append(train_prec)
    test_precision.append(test_prec)
    train_recall.append(train_rec)
    test_recall.append(test_rec)
    train_f1.append(train_f1score)
    test_f1.append(test_f1score)
    train_roc_auc_scores.append(train_roc_auc) # Changed variable name
    test_roc_auc_scores.append(test_roc_auc) # Changed variable name

    # Summarize progress
    #print('>%d, train_acc: %.3f, test_acc: %.3f, train_prec: %.3f, test_prec: %.3f, train_rec: %.3f, test_rec: %.3f, train_f1: %.3f, test_f1: %.3f, train_roc_auc: %.3f, test_roc_auc: %.3f' % (i, train_acc, test_acc, train_prec, test_prec, train_rec, test_rec, train_f1score, test_f1score, train_roc_auc, test_roc_auc))

# Plot of train and test scores vs tree depth
plt.figure(figsize=(12, 8))

plt.subplot(2, 3, 1)
plt.plot(values, train_scores, '-o', label='Train')
plt.plot(values, test_scores, '-o', label='Test')
plt.xlabel('Max Depth')
plt.ylabel('Accuracy')
plt.title('Random Forest Accuracy')
plt.legend(loc='lower right')
plt.grid(True)

plt.subplot(2, 3, 2)
plt.plot(values, train_precision, '-o', label='Train')
plt.plot(values, test_precision, '-o', label='Test')
plt.xlabel('Max Depth')
plt.ylabel('Precision')
plt.title('Random Forest Precision')
plt.legend(loc='lower right')
plt.grid(True)

plt.subplot(2, 3, 3)
plt.plot(values, train_recall, '-o', label='Train')
plt.plot(values, test_recall, '-o', label='Test')
plt.xlabel('Max Depth')
plt.ylabel('Recall')
plt.title('Random Forest Recall')
plt.legend(loc='lower right')
plt.grid(True)

plt.subplot(2, 3, 4)
plt.plot(values, train_f1, '-o', label='Train')
plt.plot(values, test_f1, '-o', label='Test')
plt.xlabel('Max Depth')
plt.ylabel('F1 Score')
plt.title('Random Forest F1 Score')
plt.legend(loc='lower right')
plt.grid(True)

plt.subplot(2, 3, 5)
plt.plot(values, train_roc_auc_scores, '-o', label='Train')
plt.plot(values, test_roc_auc_scores, '-o', label='Test')
plt.xlabel('Max Depth')
plt.ylabel('ROC AUC Score')
plt.title('Random Forest ROC AUC Score')
plt.legend(loc='lower right')
plt.grid(True)

plt.tight_layout()


# Save the plot as an image file
plt.savefig('photoDF/random_forest_performance.png')
plt.show()

```


```{python}
#| label: weather data max depth overfitting100
#| echo: false
#| message: true
#| include: false
#| eval: false


X = weather.drop(columns=['RainTomorrow'])
y = weather['RainTomorrow']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the training data
scaler.fit(X_train)

# Transform the training and testing data separately
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert scaled data back to DataFrames
X_train_scaled_weather = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled_weather = pd.DataFrame(X_test_scaled, columns=X.columns)

# Define lists to collect scores
train_scores, test_scores = [], []
train_precision, test_precision = [], []
train_recall, test_recall = [], []
train_f1, test_f1 = [], []
train_roc_auc_scores, test_roc_auc_scores = [], [] # Changed variable names

# Define the tree depths to evaluate
values = [i for i in range(1, 21)]

# Evaluate a random forest for each depth
for i in values:
    # Configure the model
    model = RandomForestClassifier(n_estimators=100, max_depth=i, random_state=42)
    
    # Fit model on the training dataset
    model.fit(X_train_scaled_weather, y_train)
    
    # Predictions
    train_yhat = model.predict(X_train_scaled_weather)
    test_yhat = model.predict(X_test_scaled_weather)
    
    # Accuracy
    train_acc = accuracy_score(y_train, train_yhat)
    test_acc = accuracy_score(y_test, test_yhat)
    
    # Precision
    train_prec = precision_score(y_train, train_yhat)
    test_prec = precision_score(y_test, test_yhat)
    
    # Recall
    train_rec = recall_score(y_train, train_yhat)
    test_rec = recall_score(y_test, test_yhat)
    
    # F1 Score
    train_f1score = f1_score(y_train, train_yhat)
    test_f1score = f1_score(y_test, test_yhat)
    
    # ROC AUC Score
    train_roc_auc = roc_auc_score(y_train, train_yhat)
    test_roc_auc = roc_auc_score(y_test, test_yhat)
    
    # Append scores to respective lists
    train_scores.append(train_acc)
    test_scores.append(test_acc)
    train_precision.append(train_prec)
    test_precision.append(test_prec)
    train_recall.append(train_rec)
    test_recall.append(test_rec)
    train_f1.append(train_f1score)
    test_f1.append(test_f1score)
    train_roc_auc_scores.append(train_roc_auc) # Changed variable name
    test_roc_auc_scores.append(test_roc_auc) # Changed variable name

    # Summarize progress
    #print('>%d, train_acc: %.3f, test_acc: %.3f, train_prec: %.3f, test_prec: %.3f, train_rec: %.3f, test_rec: %.3f, train_f1: %.3f, test_f1: %.3f, train_roc_auc: %.3f, test_roc_auc: %.3f' % (i, train_acc, test_acc, train_prec, test_prec, train_rec, test_rec, train_f1score, test_f1score, train_roc_auc, test_roc_auc))

# Plot of train and test scores vs tree depth
plt.figure(figsize=(12, 8))

plt.subplot(2, 3, 1)
plt.plot(values, train_scores, '-o', label='Train')
plt.plot(values, test_scores, '-o', label='Test')
plt.xlabel('Max Depth')
plt.ylabel('Accuracy')
plt.title('Random Forest Accuracy')
plt.legend(loc='lower right')
plt.grid(True)

plt.subplot(2, 3, 2)
plt.plot(values, train_precision, '-o', label='Train')
plt.plot(values, test_precision, '-o', label='Test')
plt.xlabel('Max Depth')
plt.ylabel('Precision')
plt.title('Random Forest Precision')
plt.legend(loc='lower right')
plt.grid(True)

plt.subplot(2, 3, 3)
plt.plot(values, train_recall, '-o', label='Train')
plt.plot(values, test_recall, '-o', label='Test')
plt.xlabel('Max Depth')
plt.ylabel('Recall')
plt.title('Random Forest Recall')
plt.legend(loc='lower right')
plt.grid(True)

plt.subplot(2, 3, 4)
plt.plot(values, train_f1, '-o', label='Train')
plt.plot(values, test_f1, '-o', label='Test')
plt.xlabel('Max Depth')
plt.ylabel('F1 Score')
plt.title('Random Forest F1 Score')
plt.legend(loc='lower right')
plt.grid(True)

plt.subplot(2, 3, 5)
plt.plot(values, train_roc_auc_scores, '-o', label='Train')
plt.plot(values, test_roc_auc_scores, '-o', label='Test')
plt.xlabel('Max Depth')
plt.ylabel('ROC AUC Score')
plt.title('Random Forest ROC AUC Score')
plt.legend(loc='lower right')
plt.grid(True)

plt.tight_layout()

# Save the plot as an image file
plt.savefig('photoDF/random_forest_performance2.png')
plt.show()

```


```{python}
#| label: weather data max depth overfitting150
#| echo: false
#| message: true
#| include: false
#| eval: false

X = weather.drop(columns=['RainTomorrow'])
y = weather['RainTomorrow']

X = weather.drop(columns=['RainTomorrow'])
y = weather['RainTomorrow']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit the scaler to the training data
scaler.fit(X_train)

# Transform the training and testing data separately
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert scaled data back to DataFrames
X_train_scaled_weather = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled_weather = pd.DataFrame(X_test_scaled, columns=X.columns)

# Define lists to collect scores
train_scores, test_scores = [], []
train_precision, test_precision = [], []
train_recall, test_recall = [], []
train_f1, test_f1 = [], []
train_roc_auc_scores, test_roc_auc_scores = [], [] # Changed variable names

# Define the tree depths to evaluate
values = [i for i in range(1, 21)]

# Evaluate a random forest for each depth
for i in values:
    # Configure the model
    model = RandomForestClassifier(n_estimators=150, max_depth=i, random_state=42)
    
    # Fit model on the training dataset
    model.fit(X_train_scaled_weather, y_train)
    
    # Predictions
    train_yhat = model.predict(X_train_scaled_weather)
    test_yhat = model.predict(X_test_scaled_weather)
    
    # Accuracy
    train_acc = accuracy_score(y_train, train_yhat)
    test_acc = accuracy_score(y_test, test_yhat)
    
    # Precision
    train_prec = precision_score(y_train, train_yhat)
    test_prec = precision_score(y_test, test_yhat)
    
    # Recall
    train_rec = recall_score(y_train, train_yhat)
    test_rec = recall_score(y_test, test_yhat)
    
    # F1 Score
    train_f1score = f1_score(y_train, train_yhat)
    test_f1score = f1_score(y_test, test_yhat)
    
    # ROC AUC Score
    train_roc_auc = roc_auc_score(y_train, train_yhat)
    test_roc_auc = roc_auc_score(y_test, test_yhat)
    
    # Append scores to respective lists
    train_scores.append(train_acc)
    test_scores.append(test_acc)
    train_precision.append(train_prec)
    test_precision.append(test_prec)
    train_recall.append(train_rec)
    test_recall.append(test_rec)
    train_f1.append(train_f1score)
    test_f1.append(test_f1score)
    train_roc_auc_scores.append(train_roc_auc) # Changed variable name
    test_roc_auc_scores.append(test_roc_auc) # Changed variable name

    # Summarize progress
    print('>%d, train_acc: %.3f, test_acc: %.3f, train_prec: %.3f, test_prec: %.3f, train_rec: %.3f, test_rec: %.3f, train_f1: %.3f, test_f1: %.3f, train_roc_auc: %.3f, test_roc_auc: %.3f' % (i, train_acc, test_acc, train_prec, test_prec, train_rec, test_rec, train_f1score, test_f1score, train_roc_auc, test_roc_auc))

# Plot of train and test scores vs tree depth
plt.figure(figsize=(12, 8))

plt.subplot(2, 3, 1)
plt.plot(values, train_scores, '-o', label='Train')
plt.plot(values, test_scores, '-o', label='Test')
plt.xlabel('Max Depth')
plt.ylabel('Accuracy')
plt.title('Random Forest Accuracy')
plt.legend(loc='lower right')
plt.grid(True)

plt.subplot(2, 3, 2)
plt.plot(values, train_precision, '-o', label='Train')
plt.plot(values, test_precision, '-o', label='Test')
plt.xlabel('Max Depth')
plt.ylabel('Precision')
plt.title('Random Forest Precision')
plt.legend(loc='lower right')
plt.grid(True)

plt.subplot(2, 3, 3)
plt.plot(values, train_recall, '-o', label='Train')
plt.plot(values, test_recall, '-o', label='Test')
plt.xlabel('Max Depth')
plt.ylabel('Recall')
plt.title('Random Forest Recall')
plt.legend(loc='lower right')
plt.grid(True)

plt.subplot(2, 3, 4)
plt.plot(values, train_f1, '-o', label='Train')
plt.plot(values, test_f1, '-o', label='Test')
plt.xlabel('Max Depth')
plt.ylabel('F1 Score')
plt.title('Random Forest F1 Score')
plt.legend(loc='lower right')
plt.grid(True)

plt.subplot(2, 3, 5)
plt.plot(values, train_roc_auc_scores, '-o', label='Train')
plt.plot(values, test_roc_auc_scores, '-o', label='Test')
plt.xlabel('Max Depth')
plt.ylabel('ROC AUC Score')
plt.title('Random Forest ROC AUC Score')
plt.legend(loc='lower right')
plt.grid(True)

plt.tight_layout()

# Save the plot as an image file
plt.savefig('photoDF/random_forest_performance3.png')
plt.show()

```

When exploring different combinations of max depth and number of estimators for the Random Forest Classifier, increasing the max depth generally led to improved performance metrics on the training set, including accuracy, precision, recall, F1-score, and ROC AUC score. However, the performance on the testing dataset showed fluctuations, with some max depths performing better than others. From the plots, it's evident that the training scores consistently improve with increasing max depth, but the testing scores fluctuate, indicating potential overfitting.

The optimal max depth appears to be in the range of 6 to 8, where the differences in performance metrics between different depths are minimal, suggesting a balance between model complexity and generalization. This range offers good performance on both the training and testing datasets while reducing the risk of overfitting.

Interestingly, varying the number of estimators 50, 100, and 150 in the Random Forest did not significantly impact the shape or trend of the learning curves. Despite differences in the number of trees in the forest, the overall behavior of the model, as reflected in the learning curves, remained consistent. This suggests that increasing the number of estimators beyond a certain point may not lead to substantial improvements in model performance. Therefore, it is very important to consider the trade-off between computational complexity and performance when selecting the number of estimators.

# REFLECTION AND CONCLUSION

## Reflection

- **Best Model Performance:** The combination of MICE imputation technique with the Random Forest (Scaled) model yielded the highest the highest values across most evaluation criteria, including accuracy, precision, F1-score, and ROC/AUC. Moreover, this model achieved superior performance while maintaining reasonable computational time, making it the best choice for rain prediction in Australia among the listed techniques and models.

- **Feature Selection Analysis:** Employing the SelectKBest algorithm to identify the most influential features did not significantly improve model performance. Potential reasons for the lack of improvement include the relevance of excluded features, absence of noise in the data, and the complexity of feature interactions.

- **Cross-Validation:** Models with and without cross-validation demonstrated similar accuracy and performance metrics, indicating stability and good generalization ability.

- **Learning Curve Analysis:** The learning curve depicted the model's performance based on varying amounts of training data. The model achieved near-perfect scores when trained on a large amount of data, suggesting a high capacity to memorize the training set. Additional samples beyond a certain point showed minimal improvement in performance, indicating diminishing returns with increased training data.

- **Overfitting Analysis:** Testing various configurations of the Random Forest Classifier revealed fluctuations in performance metrics on the testing dataset, indicating potential overfitting. Optimal model complexity was observed in the range of 6 to 8 for max depth, balancing between performance on training and testing datasets. On the other hand, the number of estimators did not significantly impact model performance beyond a certain threshold, highlighting the importance of considering the trade-off between computational complexity and performance.

## Conclusion:

In this project, we effectively explored various techniques and models for rain prediction in Australia, focusing on data preprocessing, feature selection, and model evaluation.

Despite efforts to optimize model performance with some strategies, like feature selection, which did not lead to significant improvements, the results emphasize the importance of selecting appropriate techniques and considering model complexity in machine learning tasks.

The analysis provided insights into the importance of model stability, generalization ability, and the balance between complexity and performance in machine learning tasks.

Future research could explore additional feature engineering methods, alternative algorithms, or ensemble techniques to further improve rain prediction accuracy in Australia.

# REFERENCES

A. Picornell, J. Oteros, R. Ruiz-Mata, M. Recio, M.M. Trigo, M. Martínez-Bracero, B. Lara, A. Serrano-García, C. Galán, H. García-Mozo, P. Alcázar, R. Pérez-Badia, B. Cabezudo, J. Romero-Morte, J. Rojo. (2021). Methods for interpolating missing data in aerobiological databases. Environmental Research.[https://doi.org/10.1016/j.envres.2021.111391](https://doi.org/10.1016/j.envres.2021.111391)

Alhamid, M. (2020, December 24). What is Cross-Validation? Testing your machine learning models with cross-validation. Towards Data Science. [https://towardsdatascience.com/what-is-cross-validation-60c01f9d9e75](https://towardsdatascience.com/what-is-cross-validation-60c01f9d9e75)

Ali, Aida & Shamsuddin, Siti Mariyam & Ralescu, Anca. (2015). Classification with class imbalance problem: A review. 7. 176-204. UTM Big Data Centre, Ibnu Sina Institute for Scientific and Industrial Research Universiti Teknologi Malaysia.[https://www.researchgate.net/publication/288228469_Classification_with_class_imbalance_problem_A_review](https://www.researchgate.net/publication/288228469_Classification_with_class_imbalance_problem_A_review) 

Aliferis, C., Simon, G. (2024). Overfitting, Underfitting, and General Model Overconfidence and Under-Performance Pitfalls and Best Practices in Machine Learning and AI. Artificial Intelligence and Machine Learning in Health Care and Medical Sciences. Health Informatics. Springer, Cham. [https://doi.org/10.1007/978-3-031-39355-6_10](https://doi.org/10.1007/978-3-031-39355-6_10)

Alves, L. M. (2021, July 2). KNN (K Nearest Neighbors) and KNeighborsClassifier — What it is, how it works, and a practical…[https://luis-miguel-code.medium.com/knn-k-nearest-neighbors-and-kneighborsclassifier-what-it-is-how-it-works-and-a-practical-914ec089e467](https://luis-miguel-code.medium.com/knn-k-nearest-neighbors-and-kneighborsclassifier-what-it-is-how-it-works-and-a-practical-914ec089e467)

Azur, M. J., Stuart, E. A., Frangakis, C., & Leaf, P. J. (2011). Multiple imputation by chained equations: what is it and how does it work? International journal of methods in psychiatric research, 20(1), 40–49. [https://doi.org/10.1002/mpr.329](https://doi.org/10.1002/mpr.329)

Blagec, K., Dorffner, G., Moradi, M., & Samwald, M. (2020). A critical analysis of metrics used for measuring progress in artificial intelligence. Section for Artificial Intelligence and Decision Support; Center for Medical Statistics, Informatics, and Intelligent Systems; Medical University of Vienna. [https://arxiv.org/pdf/2008.02577](https://arxiv.org/pdf/2008.02577)

Gabr, M., Ibrahim, Y., Mostafa H., & Doaa S. E. (2023). Effect of Missing Data Types and Imputation Methods on Supervised Classifiers: An Evaluation Study. Big Data and Cognitive Computing 7, no. 1: 55. [https://doi.org/10.3390/bdcc7010055](https://doi.org/10.3390/bdcc7010055)

Giola, C., Danti, P., & Magnani, S. (2021, July 13). Learning curves: A novel approach for robustness improvement of load forecasting. *MDPI.* [https://www.mdpi.com/2673-4591/5/1/38#metrics](https://www.mdpi.com/2673-4591/5/1/38#metrics)

Hameed, W. & Ali, N. (2023). Missing value imputation Techniques: A Survey. UHD Journal of Science and Technology. [https://www.researchgate.net/publication/369674417_Missing_value_imputation_Techniques_A_Survey](https://www.researchgate.net/publication/369674417_Missing_value_imputation_Techniques_A_Survey)

IBM. (2022). What Is Logistic Regression? IBM.[https://www.ibm.com/topics/logistic-regression](https://www.ibm.com/topics/logistic-regression)

IBM. (2023a). What is a Decision Tree | IBM.[https://www.ibm.com/topics/decision-trees](https://www.ibm.com/topics/decision-trees)

IBM. (2023b). What is Random Forest? | IBM.[https://www.ibm.com/topics/random-forest](https://www.ibm.com/topics/random-forest)

Jason B. (2018, November 20). A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning. Machine Learning Mastery.[https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)

Jiaxu P., Jungpil H., Ke-Wei H. (2022) Handling Missing Values in Information Systems Research: A Review of Methods and Assumptions. Information Systems Research 34(1):5-26. [https://doi.org/10.1287/isre.2022.1104](https://doi.org/10.1287/isre.2022.1104)

Kang H. (2013). The prevention and handling of the missing data. Korean Journal of Anesthesiology, 64(5), 402–406. [https://doi.org/10.4097/kjae.2013.64.5.402](https://doi.org/10.4097/kjae.2013.64.5.402)

Kavya, D. (2023, February 15). Optimizing Performance: SelectKBest for Efficient Feature Selection in Machine Learning. Medium. [https://medium.com/@Kavya2099/optimizing-performance-selectkbest-for-efficient-feature-selection-in-machine-learning-3b635905ed48](https://medium.com/@Kavya2099/optimizing-performance-selectkbest-for-efficient-feature-selection-in-machine-learning-3b635905ed48)

Makaba, T. & Dogo, E. (2019). A Comparison of Strategies for Missing Values in Data on Machine Learning Classification Algorithms. International Multidisciplinary Information Technology and Engineering Conference (IMITEC), Vanderbijlpark, South Africa, pp. 1-7. [https://ieeexplore.ieee.org/document/9015889](https://ieeexplore.ieee.org/document/9015889)

Ribeiro, D. (2023). Missing values in data analysis: A comprehensive guide. Medium. [https://medium.com/data-science-as-a-better-idea/missing-values-in-data-analysis-a-comprehensive-guide-2151bc2e8579](https://medium.com/data-science-as-a-better-idea/missing-values-in-data-analysis-a-comprehensive-guide-2151bc2e8579)

Padgett, C. & Skilbeck, C. & Summers, M. (2014). Missing Data: The 
Importance and Impact of Missing Data from Clinical Research. Brain Impairment. [https://www.researchgate.net/publication/262036960_Missing_Data_The_Importance_and_Impact_of_Missing_Data_from_Clinical_Research](https://www.researchgate.net/publication/262036960_Missing_Data_The_Importance_and_Impact_of_Missing_Data_from_Clinical_Research)

Salgado, C.M., Azevedo, C., Proença, H., & Vieira, S.M. (2016). Missing Data. In: Secondary Analysis of Electronic Health Records. Springer, Cham. [https://doi.org/10.1007/978-3-319-43742-2_13](https://doi.org/10.1007/978-3-319-43742-2_13)

Song, Q. & Shepperd, M. (2007). Missing Data Imputation Techniques. 
International Journal of Business Intelligence and Data Mining. [https://www.researchgate.net/publication/220579612_Missing_Data_Imputation_Techniques](https://www.researchgate.net/publication/220579612_Missing_Data_Imputation_Techniques)

Torres, M. & Juan, A. (2014). Comparison of imputation methods for handling missing categorical data with univariate patterns. Revista de Métodos Cuantitativos para la Economía y la Empresa, ISSN 1886-516X, Universidad Pablo de Olavide, Sevilla, Vol.17, pp. 101-120. [https://hdl.handle.net/10419/113873](https://hdl.handle.net/10419/113873)

Wizards, D. S. (2023, July 7). Understanding the AdaBoost Algorithm. Medium.[https://medium.com/@datasciencewizards/understanding-the-adaboost-algorithm-2e9344d83d9b](https://medium.com/@datasciencewizards/understanding-the-adaboost-algorithm-2e9344d83d9b)